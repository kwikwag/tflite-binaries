diff --git a/tensorflow/lite/core/c/operator.cc b/tensorflow/lite/core/c/operator.cc
index ee67da2dcb2..2ae7d71f1d3 100644
--- a/tensorflow/lite/core/c/operator.cc
+++ b/tensorflow/lite/core/c/operator.cc
@@ -32,17 +32,17 @@ TfLiteOperator* TfLiteOperatorCreate(TfLiteBuiltinOperator builtin_code,
 TfLiteOperator* TfLiteOperatorCreateWithData(TfLiteBuiltinOperator builtin_code,
                                              const char* custom_name,
                                              int version, void* user_data) {
-  return new TfLiteOperator{.custom_name = custom_name,
-                            .version = version,
-                            .init = nullptr,
-                            .free = nullptr,
-                            .prepare = nullptr,
-                            .invoke = nullptr,
-                            .async_kernel = nullptr,
-                            .builtin_code = builtin_code,
-                            .node_index = -1,
-                            .inplace_operator = kTfLiteInplaceOpNone,
-                            .user_data = user_data};
+  return new TfLiteOperator{/*custom_name=*/custom_name,
+                            /*version=*/version,
+                            /*init=*/nullptr,
+                            /*free=*/nullptr,
+                            /*prepare=*/nullptr,
+                            /*invoke=*/nullptr,
+                            /*async_kernel=*/nullptr,
+                            /*builtin_code=*/builtin_code,
+                            /*node_index=*/-1,
+                            /*inplace_operator=*/kTfLiteInplaceOpNone,
+                            /*user_data=*/user_data};
 }
 
 void TfLiteOperatorDelete(TfLiteOperator* reg) { delete reg; }
diff --git a/tensorflow/lite/delegates/coreml/builders/test_util.h b/tensorflow/lite/delegates/coreml/builders/test_util.h
index f55f7c5b890..4440d3d93c6 100644
--- a/tensorflow/lite/delegates/coreml/builders/test_util.h
+++ b/tensorflow/lite/delegates/coreml/builders/test_util.h
@@ -36,8 +36,8 @@ class SingleOpModelWithCoreMlDelegate : public tflite::SingleOpModel {
  private:
   tflite::Interpreter::TfLiteDelegatePtr delegate_;
   TfLiteCoreMlDelegateOptions params_ = {
-      .enabled_devices = TfLiteCoreMlDelegateAllDevices,
-      .min_nodes_per_partition = 1,
+      /*enabled_devices=*/TfLiteCoreMlDelegateAllDevices,
+      /*min_nodes_per_partition=*/1,
   };
 };
 
diff --git a/tensorflow/lite/delegates/gpu/common/tasks/space_to_depth_test_util.cc b/tensorflow/lite/delegates/gpu/common/tasks/space_to_depth_test_util.cc
index 036976e667a..d29e57445ce 100644
--- a/tensorflow/lite/delegates/gpu/common/tasks/space_to_depth_test_util.cc
+++ b/tensorflow/lite/delegates/gpu/common/tasks/space_to_depth_test_util.cc
@@ -45,7 +45,7 @@ absl::Status SpaceToDepthTensorShape1x2x2x1BlockSize2Test(
   TensorFloat32 src_tensor;
   src_tensor.shape = BHWC(1, 2, 2, 1);
   src_tensor.data = {half(1.0f), half(2.0f), half(3.0f), half(4.0f)};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   for (auto precision : env->GetSupportedPrecisions()) {
     auto data_type = DeduceDataTypeFromPrecision(precision);
     for (auto storage : env->GetSupportedStorages(data_type)) {
@@ -70,7 +70,7 @@ absl::Status SpaceToDepthTensorShape1x2x2x2BlockSize2Test(
   src_tensor.shape = BHWC(1, 2, 2, 2);
   src_tensor.data = {half(1.4f), half(2.3f), half(3.2f), half(4.1f),
                      half(5.4f), half(6.3f), half(7.2f), half(8.1f)};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   for (auto precision : env->GetSupportedPrecisions()) {
     auto data_type = DeduceDataTypeFromPrecision(precision);
     for (auto storage : env->GetSupportedStorages(data_type)) {
@@ -99,7 +99,7 @@ absl::Status SpaceToDepthTensorShape1x2x2x3BlockSize2Test(
   src_tensor.data = {half(1.0f), half(2.0f),  half(3.0f),  half(4.0f),
                      half(5.0f), half(6.0f),  half(7.0f),  half(8.0f),
                      half(9.0f), half(10.0f), half(11.0f), half(12.0f)};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   for (auto precision : env->GetSupportedPrecisions()) {
     auto data_type = DeduceDataTypeFromPrecision(precision);
     for (auto storage : env->GetSupportedStorages(data_type)) {
@@ -126,7 +126,7 @@ absl::Status SpaceToDepthTensorShape1x4x4x1BlockSize2Test(
                      half(3.0f),  half(4.0f),  half(7.0f),  half(8.0f),
                      half(9.0f),  half(10.0f), half(13.0f), half(14.0f),
                      half(11.0f), half(12.0f), half(15.0f), half(16.0f)};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   for (auto precision : env->GetSupportedPrecisions()) {
     auto data_type = DeduceDataTypeFromPrecision(precision);
     for (auto storage : env->GetSupportedStorages(data_type)) {
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth_test.cc b/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth_test.cc
index 0ff132b8147..7a1cc894f6b 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth_test.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/space_to_depth_test.cc
@@ -32,10 +32,10 @@ namespace {
 
 TEST(SpaceToDepthTest, TensorShape1x2x2x1BlockSize2) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 2, 1), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 2, 1), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 1, 1, 4), .ref = 1};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 1, 1, 4), /*ref=*/1};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   SingleOpModel model({ToString(OperationType::SPACE_TO_DEPTH), attr}, {input},
                       {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f, 4.0f}));
@@ -46,10 +46,10 @@ TEST(SpaceToDepthTest, TensorShape1x2x2x1BlockSize2) {
 
 TEST(SpaceToDepthTest, TensorShape1x2x2x2BlockSize2) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 2, 2), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 2, 2), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 1, 1, 8), .ref = 1};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 1, 1, 8), /*ref=*/1};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   SingleOpModel model({ToString(OperationType::SPACE_TO_DEPTH), attr}, {input},
                       {output});
   ASSERT_TRUE(model.PopulateTensor(
@@ -62,10 +62,10 @@ TEST(SpaceToDepthTest, TensorShape1x2x2x2BlockSize2) {
 
 TEST(SpaceToDepthTest, TensorShape1x2x2x3BlockSize2) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 2, 3), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 2, 3), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 1, 1, 12), .ref = 1};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 1, 1, 12), /*ref=*/1};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   SingleOpModel model({ToString(OperationType::SPACE_TO_DEPTH), attr}, {input},
                       {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f,  //
@@ -81,10 +81,10 @@ TEST(SpaceToDepthTest, TensorShape1x2x2x3BlockSize2) {
 
 TEST(SpaceToDepthTest, TensorShape1x4x4x1BlockSize2) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 4, 4, 1), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 4, 4, 1), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 2, 4), .ref = 1};
-  const SpaceToDepthAttributes attr = {.block_size = 2};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 2, 4), /*ref=*/1};
+  const SpaceToDepthAttributes attr = {/*block_size=*/2};
   SingleOpModel model({ToString(OperationType::SPACE_TO_DEPTH), attr}, {input},
                       {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0, 2.0, 5.0, 6.0,     //
diff --git a/tensorflow/lite/delegates/gpu/gl/kernels/tile_test.cc b/tensorflow/lite/delegates/gpu/gl/kernels/tile_test.cc
index b74cee9e18c..11fc0dcc674 100644
--- a/tensorflow/lite/delegates/gpu/gl/kernels/tile_test.cc
+++ b/tensorflow/lite/delegates/gpu/gl/kernels/tile_test.cc
@@ -32,9 +32,9 @@ namespace {
 
 TEST(TileTest, ChannelsTiling) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 1, 3), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 1, 3), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 1, 6), .ref = 1};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 1, 6), /*ref=*/1};
   SingleOpModel model({ToString(OperationType::TILE)}, {input}, {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f}));
   ASSERT_OK(model.Invoke(*NewTileNodeShader()));
@@ -45,9 +45,9 @@ TEST(TileTest, ChannelsTiling) {
 
 TEST(TileTest, WidthTiling) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 1, 2, 3), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 1, 2, 3), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 1, 4, 3), .ref = 1};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 1, 4, 3), /*ref=*/1};
   SingleOpModel model({ToString(OperationType::TILE)}, {input}, {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f}));
   ASSERT_OK(model.Invoke(*NewTileNodeShader()));
@@ -58,9 +58,9 @@ TEST(TileTest, WidthTiling) {
 
 TEST(TileTest, HeightTiling) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 1, 3), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 1, 3), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 4, 1, 3), .ref = 1};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 4, 1, 3), /*ref=*/1};
   SingleOpModel model({ToString(OperationType::TILE)}, {input}, {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f}));
   ASSERT_OK(model.Invoke(*NewTileNodeShader()));
@@ -71,9 +71,9 @@ TEST(TileTest, HeightTiling) {
 
 TEST(TileTest, HWCTiling) {
   const TensorRef<BHWC> input = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 2, 2, 3), .ref = 0};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 2, 2, 3), /*ref=*/0};
   const TensorRef<BHWC> output = {
-      .type = DataType::FLOAT32, .shape = BHWC(1, 4, 4, 6), .ref = 1};
+      /*type=*/DataType::FLOAT32, /*shape=*/BHWC(1, 4, 4, 6), /*ref=*/1};
   SingleOpModel model({ToString(OperationType::TILE)}, {input}, {output});
   ASSERT_TRUE(model.PopulateTensor(0, {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f,
                                        8.0f, 9.0f, 10.0f, 11.0f, 12.0f}));
diff --git a/tensorflow/lite/delegates/gpu/gl/runtime.cc b/tensorflow/lite/delegates/gpu/gl/runtime.cc
index fe3db9547fd..7bddef46f06 100644
--- a/tensorflow/lite/delegates/gpu/gl/runtime.cc
+++ b/tensorflow/lite/delegates/gpu/gl/runtime.cc
@@ -92,10 +92,10 @@ absl::Status MakeGlTexture(const Object& object, const ObjectData& data,
       }
       return std::visit(
           TextureF16Maker{
-              .data = absl::MakeConstSpan(
+              /*data=*/absl::MakeConstSpan(
                   reinterpret_cast<const uint16_t*>(data.data()),
                   data.size() / 2),
-              .gl_texture = gl_texture,
+              /*gl_texture=*/gl_texture,
           },
           object.size);
     }
@@ -105,10 +105,10 @@ absl::Status MakeGlTexture(const Object& object, const ObjectData& data,
       }
       return std::visit(
           TextureF32Maker{
-              .data = absl::MakeConstSpan(
+              /*data=*/absl::MakeConstSpan(
                   reinterpret_cast<const float*>(data.data()),
                   data.size() / sizeof(float)),
-              .gl_texture = gl_texture,
+              /*gl_texture=*/gl_texture,
           },
           object.size);
     }
diff --git a/tensorflow/lite/delegates/gpu/metal/benchmarking/main.mm b/tensorflow/lite/delegates/gpu/metal/benchmarking/main.mm
index 809844f6d14..665189b325c 100644
--- a/tensorflow/lite/delegates/gpu/metal/benchmarking/main.mm
+++ b/tensorflow/lite/delegates/gpu/metal/benchmarking/main.mm
@@ -371,7 +371,7 @@ class DelegateContext {
 
 TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate) {
   const TfLiteRegistration kRegistration = {
-      .init = [](TfLiteContext* context, const char* buffer, size_t) -> void* {
+      /*init=*/[](TfLiteContext* context, const char* buffer, size_t) -> void* {
         auto* delegate_context = new DelegateContext();
         if (!delegate_context->Init(context,
                                     reinterpret_cast<const TfLiteDelegateParams*>(buffer))) {
@@ -380,13 +380,13 @@ TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate) {
         }
         return delegate_context;
       },
-      .free = [](TfLiteContext* context, void* buffer) -> void {
+      /*free=*/[](TfLiteContext* context, void* buffer) -> void {
         delete reinterpret_cast<DelegateContext*>(buffer);
       },
-      .prepare = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
+      /*prepare=*/[](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
         return node->user_data ? kTfLiteOk : kTfLiteError;
       },
-      .invoke = nullptr,
+      /*invoke=*/nullptr,
   };
 
   TfLiteIntArray* ops_to_replace = GetOpsToReplace(context);
diff --git a/tensorflow/lite/delegates/gpu/metal_delegate.mm b/tensorflow/lite/delegates/gpu/metal_delegate.mm
index 1a0897c221d..afcd4714f01 100644
--- a/tensorflow/lite/delegates/gpu/metal_delegate.mm
+++ b/tensorflow/lite/delegates/gpu/metal_delegate.mm
@@ -738,9 +738,9 @@ bool TFLGpuDelegateSetCommandBuffer(TfLiteDelegate* delegate,
 
 TFLGpuDelegateOptions TFLGpuDelegateOptionsDefault() {
   TFLGpuDelegateOptions options = {
-      .allow_precision_loss = false,
-      .wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,
-      .enable_quantization = true,
+      /*allow_precision_loss=*/false,
+      /*wait_type=*/TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,
+      /*enable_quantization=*/true,
   };
   return options;
 }
diff --git a/tensorflow/lite/delegates/hexagon/builders/conv_2d_helpers.cc b/tensorflow/lite/delegates/hexagon/builders/conv_2d_helpers.cc
index 58c7bd76fb0..53945287a8b 100644
--- a/tensorflow/lite/delegates/hexagon/builders/conv_2d_helpers.cc
+++ b/tensorflow/lite/delegates/hexagon/builders/conv_2d_helpers.cc
@@ -186,8 +186,8 @@ void Conv2dOpBuilder::SplitWeightsForDwConv(
   int weights_width_size = weight_shape_[1];
   // Split the weight tensor into 32 channel batches.
   SplitParams split_params{
-      .num_split = static_cast<uint16_t>(per_channel_quant_.splits),
-      .axis = 2,
+      /*num_split=*/static_cast<uint16_t>(per_channel_quant_.splits),
+      /*axis=*/2,
   };
   std::vector<RuntimeShape> split_shapes;
   std::vector<const tflite::RuntimeShape*> split_shapes_data;
diff --git a/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc b/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc
index a9a5cd1a784..2a46686d58a 100644
--- a/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc
+++ b/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc
@@ -519,11 +519,11 @@ ANeuralNetworksOperandType ConvertTensorTypeToNNType(
     tensor_dims = &scalar_rank;
   }
   ANeuralNetworksOperandType nn_operand_type{
-      .type = nn_type,
-      .dimensionCount = tensor_rank,
-      .dimensions = tensor_dims,
-      .scale = scale,
-      .zeroPoint = zero_point,
+      /*type=*/nn_type,
+      /*dimensionCount=*/tensor_rank,
+      /*dimensions=*/tensor_dims,
+      /*scale=*/scale,
+      /*zeroPoint=*/zero_point,
   };
   return nn_operand_type;
 }
@@ -1526,7 +1526,7 @@ class NNAPIOpBuilder {
     const TfLiteTensor* tensor = &context_->tensors[tensor_index];
     TF_LITE_ENSURE_EQ(context_, NumElements(tensor), 1);
 
-    ANeuralNetworksOperandType operand_type{.type = nn_type};
+    ANeuralNetworksOperandType operand_type{/*type=*/nn_type};
     RETURN_TFLITE_ERROR_IF_NN_ERROR_FOR_TENSOR(
         context_,
         nnapi_->ANeuralNetworksModel_addOperand(nn_model_, &operand_type),
@@ -1679,7 +1679,7 @@ class NNAPIOpBuilder {
 
   template <typename T>
   TfLiteStatus AddScalarOperand(T value, int32_t nn_type) {
-    ANeuralNetworksOperandType operand_type{.type = nn_type};
+    ANeuralNetworksOperandType operand_type{/*type=*/nn_type};
     RETURN_TFLITE_ERROR_IF_NN_ERROR(
         context_,
         nnapi_->ANeuralNetworksModel_addOperand(nn_model_, &operand_type),
@@ -1698,11 +1698,11 @@ class NNAPIOpBuilder {
   TfLiteStatus AddVectorOperand(const T* values, uint32_t num_values,
                                 int32_t nn_type, float scale,
                                 int32_t zero_point) {
-    ANeuralNetworksOperandType operand_type{.type = nn_type,
-                                            .dimensionCount = 1,
-                                            .dimensions = &num_values,
-                                            .scale = scale,
-                                            .zeroPoint = zero_point};
+    ANeuralNetworksOperandType operand_type{/*type=*/nn_type,
+                                            /*dimensionCount=*/1,
+                                            /*dimensions=*/&num_values,
+                                            /*scale=*/scale,
+                                            /*zeroPoint=*/zero_point};
 
     RETURN_TFLITE_ERROR_IF_NN_ERROR(
         context_,
@@ -1740,11 +1740,11 @@ class NNAPIOpBuilder {
                                          int32_t zero_point,
                                          int* ann_index_out) {
     ANeuralNetworksOperandType operand_type{
-        .type = nn_type,
-        .dimensionCount = dimension_count,
-        .dimensions = dimension_data,
-        .scale = scale,
-        .zeroPoint = zero_point,
+        /*type=*/nn_type,
+        /*dimensionCount=*/dimension_count,
+        /*dimensions=*/dimension_data,
+        /*scale=*/scale,
+        /*zeroPoint=*/zero_point,
     };
     RETURN_TFLITE_ERROR_IF_NN_ERROR(
         context_,
@@ -1841,11 +1841,11 @@ class NNAPIOpBuilder {
           if (quantization_params->scale->size > 1 || force_per_channel) {
             // Set up per-channel quantization.
             ann_perchannel_params = {
-                .channelDim = static_cast<uint32_t>(
+                /*channelDim=*/static_cast<uint32_t>(
                     quantization_params->quantized_dimension),
-                .scaleCount =
+                /*scaleCount=*/
                     static_cast<uint32_t>(quantization_params->scale->size),
-                .scales = quantization_params->scale->data,
+                /*scales=*/quantization_params->scale->data,
             };
             nn_type = ANEURALNETWORKS_TENSOR_QUANT8_SYMM_PER_CHANNEL;
             scale = 0.0f;
@@ -3051,7 +3051,7 @@ bool NNAPIDelegateKernel::Validate(
              NNAPIValidationFailureType::kUnsupportedOperandRank,
              "SVDF does not support rank > 1 on NNAPI 1.0.", &val_ctx);
       Expect(context->tensors[node->inputs->data[/*kWeightsFeatureTensor*/ 1]]
-                     .type == kTfLiteFloat32,
+                     /*type=*/= kTfLiteFloat32,
              NNAPIValidationFailureType::kUnsupportedInputType,
              "Weights should be Float32", &val_ctx);
     } break;
@@ -3102,7 +3102,7 @@ bool NNAPIDelegateKernel::Validate(
 
         auto is_const_tensor = [&node, &context](int tensor_idx) {
           return context->tensors[node->inputs->data[tensor_idx]]
-                     .allocation_type == kTfLiteMmapRo;
+                     /*allocation_type=*/= kTfLiteMmapRo;
         };
 
         Expect(is_const_tensor(2 /* kInputWeights */),
@@ -6912,7 +6912,7 @@ TfLiteStatus StatefulNnApiDelegate::DoPrepare(TfLiteContext* context,
   // NN API Delegate Registration (the pseudo kernel that will invoke NN
   // API node sub sets)
   static const TfLiteRegistration nnapi_delegate_kernel = {
-      .init = [](TfLiteContext* context, const char* buffer,
+      /*init=*/[](TfLiteContext* context, const char* buffer,
                  size_t length) -> void* {
         const TfLiteDelegateParams* params =
             reinterpret_cast<const TfLiteDelegateParams*>(buffer);
@@ -6931,11 +6931,11 @@ TfLiteStatus StatefulNnApiDelegate::DoPrepare(TfLiteContext* context,
         return kernel_state;
       },
 
-      .free = [](TfLiteContext* context, void* buffer) -> void {
+      /*free=*/[](TfLiteContext* context, void* buffer) -> void {
         delete reinterpret_cast<NNAPIDelegateKernel*>(buffer);
       },
 
-      .prepare = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
+      /*prepare=*/[](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
         NNAPIDelegateKernel* state =
             reinterpret_cast<NNAPIDelegateKernel*>(node->user_data);
         int* nnapi_errno =
@@ -6943,7 +6943,7 @@ TfLiteStatus StatefulNnApiDelegate::DoPrepare(TfLiteContext* context,
         return state->Prepare(context, node, nnapi_errno);
       },
 
-      .invoke = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
+      /*invoke=*/[](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
         NNAPIDelegateKernel* state =
             reinterpret_cast<NNAPIDelegateKernel*>(node->user_data);
         int* nnapi_errno =
@@ -6951,10 +6951,10 @@ TfLiteStatus StatefulNnApiDelegate::DoPrepare(TfLiteContext* context,
         return state->Invoke(context, node, nnapi_errno);
       },
 
-      .profiling_string = nullptr,
-      .builtin_code = kTfLiteBuiltinDelegate,
-      .custom_name = "TfLiteNnapiDelegate",
-      .version = 1,
+      /*profiling_string=*/nullptr,
+      /*builtin_code=*/kTfLiteBuiltinDelegate,
+      /*custom_name=*/"TfLiteNnapiDelegate",
+      /*version=*/1,
   };
 
   // Initialize caching, if applicable, from Options.
diff --git a/tensorflow/lite/delegates/nnapi/nnapi_delegate_device_selection_test.cc b/tensorflow/lite/delegates/nnapi/nnapi_delegate_device_selection_test.cc
index 6a8ad7a8c28..d63a6d81e67 100644
--- a/tensorflow/lite/delegates/nnapi/nnapi_delegate_device_selection_test.cc
+++ b/tensorflow/lite/delegates/nnapi/nnapi_delegate_device_selection_test.cc
@@ -882,12 +882,12 @@ class LongIdentityModel : public MultiOpModel, public AcceleratedModel {
   // Return the registration of a custom node simply copying input to output.
   TfLiteRegistration* CustomNoOpNode() {
     static TfLiteRegistration no_op = {
-        .init = [](TfLiteContext* context, const char* buffer,
+        /*init=*/[](TfLiteContext* context, const char* buffer,
                    size_t length) -> void* { return nullptr; },
 
-        .free = [](TfLiteContext* context, void* buffer) -> void {},
+        /*free=*/[](TfLiteContext* context, void* buffer) -> void {},
 
-        .prepare = [](TfLiteContext* context,
+        /*prepare=*/[](TfLiteContext* context,
                       TfLiteNode* node) -> TfLiteStatus {
           if (node->inputs->size != 1 || node->outputs->size != 1) {
             return kTfLiteError;
@@ -896,7 +896,7 @@ class LongIdentityModel : public MultiOpModel, public AcceleratedModel {
           return kTfLiteOk;
         },
 
-        .invoke = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
+        /*invoke=*/[](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
           auto input_tensor = context->tensors[node->inputs->data[0]];
           auto output_tensor = context->tensors[node->outputs->data[0]];
 
@@ -907,10 +907,10 @@ class LongIdentityModel : public MultiOpModel, public AcceleratedModel {
           return kTfLiteOk;
         },
 
-        .profiling_string = nullptr,
-        .builtin_code = kTfLiteBuiltinDelegate,
-        .custom_name = "NoOpTestDelegate",
-        .version = 1,
+        /*profiling_string=*/nullptr,
+        /*builtin_code=*/kTfLiteBuiltinDelegate,
+        /*custom_name=*/"NoOpTestDelegate",
+        /*version=*/1,
     };
 
     return &no_op;
diff --git a/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc b/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc
index 362be8c865e..1a6881d1f59 100644
--- a/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc
+++ b/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc
@@ -5561,11 +5561,11 @@ class NnapiTestVendorPlugin : public NnapiDelegateVendorPlugin {
     TfLiteTensor* tensor = &context->tensors[tensor_index];
     auto dimensions = GetNNAPIDimensions(tensor);
     ANeuralNetworksOperandType operand_type{
-        .type = ANEURALNETWORKS_TENSOR_FLOAT32,
-        .dimensionCount = static_cast<uint32_t>(dimensions.size()),
-        .dimensions = dimensions.data(),
-        .scale = 0.0f,
-        .zeroPoint = 0,
+        /*type=*/ANEURALNETWORKS_TENSOR_FLOAT32,
+        /*dimensionCount=*/static_cast<uint32_t>(dimensions.size()),
+        /*dimensions=*/dimensions.data(),
+        /*scale=*/0.0f,
+        /*zeroPoint=*/0,
     };
     EXPECT_EQ(NnApiImplementation()->ANeuralNetworksModel_addOperand(
                   model, &operand_type),
diff --git a/tensorflow/lite/delegates/xnnpack/weight_cache.h b/tensorflow/lite/delegates/xnnpack/weight_cache.h
index 0eb66308a37..71780650515 100644
--- a/tensorflow/lite/delegates/xnnpack/weight_cache.h
+++ b/tensorflow/lite/delegates/xnnpack/weight_cache.h
@@ -273,13 +273,13 @@ class MMapWeightCacheProvider {
 
   // Cache provider implementation for XNNPack.
   xnn_weights_cache_provider cache_provider_{
-      .context = this,
-      .look_up = MMapWeightCacheProvider::look_up,
-      .reserve_space = MMapWeightCacheProvider::reserve_space,
-      .look_up_or_insert = MMapWeightCacheProvider::look_up_or_insert,
-      .is_finalized = MMapWeightCacheProvider::is_finalized,
-      .offset_to_addr = MMapWeightCacheProvider::offset_to_addr,
-      .delete_cache = MMapWeightCacheProvider::delete_cache};
+      /*context=*/this,
+      /*look_up=*/MMapWeightCacheProvider::look_up,
+      /*reserve_space=*/MMapWeightCacheProvider::reserve_space,
+      /*look_up_or_insert=*/MMapWeightCacheProvider::look_up_or_insert,
+      /*is_finalized=*/MMapWeightCacheProvider::is_finalized,
+      /*offset_to_addr=*/MMapWeightCacheProvider::offset_to_addr,
+      /*delete_cache=*/MMapWeightCacheProvider::delete_cache};
 
   // Path to the cache file.
   std::string file_path_;
diff --git a/tensorflow/lite/delegates/xnnpack/weight_cache_test.cc b/tensorflow/lite/delegates/xnnpack/weight_cache_test.cc
index 091cf796421..2bd0859d5ba 100644
--- a/tensorflow/lite/delegates/xnnpack/weight_cache_test.cc
+++ b/tensorflow/lite/delegates/xnnpack/weight_cache_test.cc
@@ -329,18 +329,18 @@ struct FakeContext {
   // Creates a look up key for the XNNPack weight provider C interface.
   xnn_weights_cache_look_up_key LookUpKey(const uint32_t algorithm_seed,
                                           const int weights_index) const {
-    return {.seed = algorithm_seed,
-            .kernel = buffers[weights_index].data(),
-            .bias = nullptr};
+    return {/*seed=*/algorithm_seed,
+            /*kernel=*/buffers[weights_index].data(),
+            /*bias=*/nullptr};
   }
 
   // Creates a look up key for the XNNPack weight provider C interface.
   xnn_weights_cache_look_up_key LookUpKey(const uint32_t algorithm_seed,
                                           const int weights_index,
                                           const int bias_index) const {
-    return {.seed = algorithm_seed,
-            .kernel = buffers[weights_index].data(),
-            .bias = buffers[bias_index].data()};
+    return {/*seed=*/algorithm_seed,
+            /*kernel=*/buffers[weights_index].data(),
+            /*bias=*/buffers[bias_index].data()};
   }
 
   // Helps creating fake packed data.
@@ -624,9 +624,9 @@ TEST(MMapWeightCacheProviderTest, XnnpackCApiJourney) {
                                         tensor_buffer_identifiers);
 
     const xnn_weights_cache_look_up_key look_up_key_1{
-        .seed = fake_packing_algo_seed,
-        .kernel = tensors[0].data.data,
-        .bias = tensors[1].data.data};
+        /*seed=*/fake_packing_algo_seed,
+        /*kernel=*/tensors[0].data.data,
+        /*bias=*/tensors[1].data.data};
 
     // Lookup non-packed tensor.
     ASSERT_EQ(cache->look_up(cache, &look_up_key_1), SIZE_MAX);
@@ -648,9 +648,9 @@ TEST(MMapWeightCacheProviderTest, XnnpackCApiJourney) {
 
     // Add a tensor without reserving before.
     const xnn_weights_cache_look_up_key look_up_key_2{
-        .seed = fake_packing_algo_seed,
-        .kernel = tensors[2].data.data,
-        .bias = tensors[3].data.data};
+        /*seed=*/fake_packing_algo_seed,
+        /*kernel=*/tensors[2].data.data,
+        /*bias=*/tensors[3].data.data};
     const size_t build_offset_2 = cache->look_up_or_insert(
         cache, &look_up_key_2, (void*)packed_data_ref_2,
         bytes(packed_data_ref_2));
@@ -697,14 +697,14 @@ TEST(MMapWeightCacheProviderTest, XnnpackCApiJourney) {
                                         tensor_buffer_identifiers);
 
     const xnn_weights_cache_look_up_key look_up_key_1{
-        .seed = fake_packing_algo_seed,
-        .kernel = tensors[0].data.data,
-        .bias = tensors[1].data.data};
+        /*seed=*/fake_packing_algo_seed,
+        /*kernel=*/tensors[0].data.data,
+        /*bias=*/tensors[1].data.data};
 
     const xnn_weights_cache_look_up_key look_up_key_2{
-        .seed = fake_packing_algo_seed,
-        .kernel = tensors[2].data.data,
-        .bias = tensors[3].data.data};
+        /*seed=*/fake_packing_algo_seed,
+        /*kernel=*/tensors[2].data.data,
+        /*bias=*/tensors[3].data.data};
 
     ASSERT_TRUE(cache->is_finalized(cache));
 
diff --git a/tensorflow/lite/experimental/acceleration/compatibility/gpu_compatibility_test.cc b/tensorflow/lite/experimental/acceleration/compatibility/gpu_compatibility_test.cc
index d7d4538c94c..b612903cf21 100644
--- a/tensorflow/lite/experimental/acceleration/compatibility/gpu_compatibility_test.cc
+++ b/tensorflow/lite/experimental/acceleration/compatibility/gpu_compatibility_test.cc
@@ -79,8 +79,8 @@ TEST_F(GPUCompatibilityTest, ReturnsUnknownStatus) {
 TEST_F(GPUCompatibilityTest, ReturnsSupportedForFullMatch) {
   ASSERT_TRUE(list_ != nullptr);
 
-  tflite::acceleration::AndroidInfo android_info = {.android_sdk_version = "24",
-                                                    .model = "m712c"};
+  tflite::acceleration::AndroidInfo android_info = {/*android_sdk_version=*/"24",
+                                                    /*model=*/"m712c"};
 
   tflite::gpu::GpuInfo tflite_gpu_info;
   tflite_gpu_info.opengl_info.major_version = 3;
@@ -92,10 +92,10 @@ TEST_F(GPUCompatibilityTest, ReturnsSupportedForFullMatch) {
 TEST_F(GPUCompatibilityTest, ReturnsUnsupportedForFullMatch) {
   ASSERT_TRUE(list_ != nullptr);
 
-  tflite::acceleration::AndroidInfo android_info = {.android_sdk_version = "28",
-                                                    .model = "SM-G960F",
-                                                    .device = "starlte",
-                                                    .manufacturer = "Samsung"};
+  tflite::acceleration::AndroidInfo android_info = {/*android_sdk_version=*/"28",
+                                                    /*model=*/"SM-G960F",
+                                                    /*device=*/"starlte",
+                                                    /*manufacturer=*/"Samsung"};
   tflite::gpu::GpuInfo tflite_gpu_info;
   tflite_gpu_info.opengl_info.renderer_name = "Mali-G72";
   tflite_gpu_info.opengl_info.major_version = 3;
diff --git a/tensorflow/lite/experimental/acceleration/mini_benchmark/jpeg_header_parser_test.cc b/tensorflow/lite/experimental/acceleration/mini_benchmark/jpeg_header_parser_test.cc
index f3600094ae7..95ed67612d8 100644
--- a/tensorflow/lite/experimental/acceleration/mini_benchmark/jpeg_header_parser_test.cc
+++ b/tensorflow/lite/experimental/acceleration/mini_benchmark/jpeg_header_parser_test.cc
@@ -103,7 +103,7 @@ TEST(ApplyHeaderToImage, ReturnsNewImageWithDifferentHeader) {
       static_cast<size_t>(g_tflite_acceleration_chessboard_jpeg_len)};
 
   JpegHeader new_header{
-      .height = 20, .width = 30, .channels = 1, .bits_per_sample = 3};
+      /*height=*/20, /*width=*/30, /*channels=*/1, /*bits_per_sample=*/3};
 
   std::string new_image_data;
 
diff --git a/tensorflow/lite/experimental/acceleration/mini_benchmark/libjpeg_decoder_test.cc b/tensorflow/lite/experimental/acceleration/mini_benchmark/libjpeg_decoder_test.cc
index 15dc12c4e87..acea42e9f34 100644
--- a/tensorflow/lite/experimental/acceleration/mini_benchmark/libjpeg_decoder_test.cc
+++ b/tensorflow/lite/experimental/acceleration/mini_benchmark/libjpeg_decoder_test.cc
@@ -41,7 +41,7 @@ using testing::IsEmpty;
 using testing::NotNull;
 
 constexpr JpegHeader kExpectedImageDimensions{
-    .height = 300, .width = 250, .channels = 3};
+    /*height=*/300, /*width=*/250, /*channels=*/3};
 
 constexpr int kDecodedSize = kExpectedImageDimensions.height *
                              kExpectedImageDimensions.width *
@@ -143,7 +143,7 @@ TEST(LibjpegDecoderTest, DecodingFailsWhenImageDimensionsDifferFromExpected) {
   unsigned char decoded[kDecodedSize];
 
   status = decoder->DecodeImage(string_ref,
-                                {.height = 300, .width = 250, .channels = 1},
+                                {/*height=*/300, /*width=*/250, /*channels=*/1},
                                 decoded, kDecodedSize);
   EXPECT_EQ(status.code, kTfLiteError);
   EXPECT_EQ(
@@ -164,9 +164,9 @@ TEST(LibjpegDecoderTest, DecodingFailsWhenImageDimensionsAreOverThreshold) {
   tflite::StringRef origin_string_ref = {encoded.c_str(), encoded.length()};
 
   const JpegHeader kHeader{
-      .height = static_cast<int>(LibjpegDecoder::kMaxImageHeight + 1),
-      .width = static_cast<int>(LibjpegDecoder::kMaxImageWidth + 1),
-      .channels = 3};
+      /*height=*/static_cast<int>(LibjpegDecoder::kMaxImageHeight + 1),
+      /*width=*/static_cast<int>(LibjpegDecoder::kMaxImageWidth + 1),
+      /*channels=*/3};
 
   const size_t decoded_size = static_cast<size_t>(kHeader.height) *
                               static_cast<size_t>(kHeader.width) *
@@ -204,7 +204,7 @@ TEST(LibjpegDecoderTest, DecodingFailsWhenImageHasUnsupportedNumberOfChannels) {
   tflite::StringRef string_ref = {encoded.c_str(), encoded.length()};
   unsigned char decoded[300 * 250 * 4];
 
-  const JpegHeader kHeader{.height = 300, .width = 250, .channels = 4};
+  const JpegHeader kHeader{/*height=*/300, /*width=*/250, /*channels=*/4};
   status = decoder->DecodeImage(string_ref, kHeader, decoded, kDecodedSize);
   EXPECT_EQ(status.code, kTfLiteError);
   EXPECT_EQ(status.error_message,
@@ -225,7 +225,7 @@ TEST(LibjpegDecoderTest, DecodingFailsWhenExpectedBitPerSampleIsNot8) {
 
   status = decoder->DecodeImage(
       string_ref,
-      {.height = 300, .width = 250, .channels = 3, .bits_per_sample = 4},
+      {/*height=*/300, /*width=*/250, /*channels=*/3, /*bits_per_sample=*/4},
       decoded, kDecodedSize);
   EXPECT_EQ(status.code, kTfLiteError);
   EXPECT_EQ(status.error_message,
@@ -245,9 +245,9 @@ TEST(LibjpegDecoderTest, DoesNotDecodeBeyondWhatIsSpecifiedInHeader) {
                                          origin_encoded_img.length()};
 
   JpegHeader undersized_image_header = {
-      .height = kExpectedImageDimensions.height / 2,
-      .width = kExpectedImageDimensions.width / 2,
-      .channels = kExpectedImageDimensions.channels};
+      /*height=*/kExpectedImageDimensions.height / 2,
+      /*width=*/kExpectedImageDimensions.width / 2,
+      /*channels=*/kExpectedImageDimensions.channels};
   std::string altered_image;
   Status alter_header_status = BuildImageWithNewHeader(
       origin_string_ref, undersized_image_header, altered_image);
@@ -276,9 +276,9 @@ TEST(LibjpegDecoderTest, CanReadImagesWithVeryLargeRows) {
                                          origin_encoded_img.length()};
 
   JpegHeader one_long_row_image_header = {
-      .height = 1,
-      .width = static_cast<int>(LibjpegDecoder::kMaxImageWidth),
-      .channels = kExpectedImageDimensions.channels};
+      /*height=*/1,
+      /*width=*/static_cast<int>(LibjpegDecoder::kMaxImageWidth),
+      /*channels=*/kExpectedImageDimensions.channels};
   std::string altered_image;
   Status alter_header_status = BuildImageWithNewHeader(
       origin_string_ref, one_long_row_image_header, altered_image);
diff --git a/tensorflow/lite/experimental/shlo/legacy/bench/shlo_benchmark.cc b/tensorflow/lite/experimental/shlo/legacy/bench/shlo_benchmark.cc
index 4d37f2f5214..8f76afe61e1 100644
--- a/tensorflow/lite/experimental/shlo/legacy/bench/shlo_benchmark.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/bench/shlo_benchmark.cc
@@ -73,7 +73,7 @@ template <absl::Status (*op)(const QuantizedTensor&, QuantizedTensor&),
           ElementType storage_type, ElementType expressed_type, int size>
 void BM_SHLO(::benchmark::State& state) {
   Shape shape = {size};
-  QuantizedParameter quantized_parameter = {.scale = 0.1, .zero_point = 0};
+  QuantizedParameter quantized_parameter = {/*scale=*/0.1, /*zero_point=*/0};
 
   using ET = typename Storage<expressed_type>::Type;
 
@@ -108,7 +108,7 @@ template <absl::Status (*op)(const QuantizedTensor&, const QuantizedTensor&,
           ElementType storage_type, ElementType expressed_type, int size>
 void BM_SHLO(::benchmark::State& state) {
   Shape shape = {size};
-  QuantizedParameter quantized_parameter = {.scale = 0.1, .zero_point = 0};
+  QuantizedParameter quantized_parameter = {/*scale=*/0.1, /*zero_point=*/0};
 
   using ET = typename Storage<expressed_type>::Type;
 
diff --git a/tensorflow/lite/experimental/shlo/legacy/bench/xnn_benchmark.cc b/tensorflow/lite/experimental/shlo/legacy/bench/xnn_benchmark.cc
index 6e9d502692c..e7c6021cd69 100644
--- a/tensorflow/lite/experimental/shlo/legacy/bench/xnn_benchmark.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/bench/xnn_benchmark.cc
@@ -109,10 +109,10 @@ void BM_XNN_HELPER(::benchmark::State& state, Op op, size_t size) {
   external_values.reserve(num_operands + 1);
   for (auto i = 0; i < num_operands; ++i) {
     external_values.push_back(
-        {.id = static_cast<uint32_t>(i), .data = operand_values[i].data()});
+        {/*id=*/static_cast<uint32_t>(i), /*data=*/operand_values[i].data()});
   }
-  external_values.push_back({.id = static_cast<uint32_t>(num_operands),
-                             .data = result_values.data()});
+  external_values.push_back({/*id=*/static_cast<uint32_t>(num_operands),
+                             /*data=*/result_values.data()});
 
   xnn_status status = xnn_initialize(/*allocator=*/nullptr);
   if (status != xnn_status_success) {
diff --git a/tensorflow/lite/experimental/shlo/legacy/src/shlo.cc b/tensorflow/lite/experimental/shlo/legacy/src/shlo.cc
index d5b47bc2837..8df2e8e37f8 100644
--- a/tensorflow/lite/experimental/shlo/legacy/src/shlo.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/src/shlo.cc
@@ -108,14 +108,14 @@ QuantizedTensorType QuantizedTensor::baseline_type() const {
   if (is_per_tensor_quantized()) {
     QuantizedTensorElementType element_type(
         storage_type(), expressed_type(),
-        QuantizedParameter{.scale = 1.0, .zero_point = 0},
+        QuantizedParameter{/*scale=*/1.0, /*zero_point=*/0},
         type_.element_type().storage_min(), type_.element_type().storage_max());
     return QuantizedTensorType(std::move(shape), std::move(element_type));
 
   } else {
     auto n = shape.dim(*type_.element_type().quantized_dimension());
     std::vector<QuantizedParameter> parameters(n,
-                                               {.scale = 1.0, .zero_point = 0});
+                                               {/*scale=*/1.0, /*zero_point=*/0});
     QuantizedTensorElementType element_type(
         storage_type(), expressed_type(), std::move(parameters),
         type_.element_type().storage_min(), type_.element_type().storage_max());
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/broadcast_in_dim_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/broadcast_in_dim_test.cc
index b8bc9a22bf1..0e422d81446 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/broadcast_in_dim_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/broadcast_in_dim_test.cc
@@ -113,33 +113,33 @@ TEST(BroadcastInDim, Unquantized) {
 
 TEST(BroadcastInDim, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
+      {/*scale=*/0.1, /*zero_point=*/0}, {1, 3}, {1, 2, 3}, {2, 1}, {2, 3, 2},
       {1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3});
 }
 
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/clamp_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/clamp_test.cc
index 001fd6f761d..09c84c5880c 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/clamp_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/clamp_test.cc
@@ -118,53 +118,53 @@ TEST(Clamp, Unquantized) {
 
 TEST(Clamp, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
+      {/*scale=*/0.1, /*zero_point=*/0}, {3}, {0}, {-2, 0, 2}, {1}, {0, 0, 1});
 
-  test<ElementType::kSI8, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI8, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {3}, {0, 1, 1}, {-3, 0, 3},
                                               {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI8, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI8, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                              {3}, {0, 1, 1}, {-3, 0, 3},
                                              {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI8, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI8, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                              {3}, {0, 1, 1}, {-3, 0, 3},
                                              {1, 1, 2}, {0, 1, 2});
 
-  test<ElementType::kSI16, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI16, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                                {3}, {0, 1, 1}, {-3, 0, 3},
                                                {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI16, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI16, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {3}, {0, 1, 1}, {-3, 0, 3},
                                               {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI16, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI16, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                               {3}, {0, 1, 1}, {-3, 0, 3},
                                               {1, 1, 2}, {0, 1, 2});
 
-  test<ElementType::kSI32, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI32, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                                {3}, {0, 1, 1}, {-3, 0, 3},
                                                {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI32, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI32, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {3}, {0, 1, 1}, {-3, 0, 3},
                                               {1, 1, 2}, {0, 1, 2});
-  test<ElementType::kSI32, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI32, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                               {3}, {0, 1, 1}, {-3, 0, 3},
                                               {1, 1, 2}, {0, 1, 2});
 }
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/concatenate_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/concatenate_test.cc
index 3494ad9940a..98481b809b6 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/concatenate_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/concatenate_test.cc
@@ -151,80 +151,80 @@ TEST(Concatenate, Unquantized) {
 
 TEST(Concatenate, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{3, 2}, {1, 2, 3, 4, 5, 6}}, {{1, 2}, {7, 8}}}, 0,
       {{4, 2}, {1, 2, 3, 4, 5, 6, 7, 8}});
 
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0},
       {{{2, 3}, {1, 2, 3, 4, 5, 6}}, {{2, 1}, {7, 8}}}, 1,
       {{2, 4}, {1, 2, 3, 7, 4, 5, 6, 8}});
 }
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/elementwise_binary_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/elementwise_binary_test.cc
index 18714f25ae4..e846a510f94 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/elementwise_binary_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/elementwise_binary_test.cc
@@ -107,31 +107,31 @@ TEST(ElementwiseBinary, Add) {
 
 TEST(ElementwiseBinary, AddQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Add, {4}, {.scale = 1, .zero_point = 0}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/1, /*zero_point=*/0}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Add, {4}, {.scale = 2, .zero_point = 2}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/2, /*zero_point=*/2}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Add, {4}, {.scale = 0.5, .zero_point = -10}, {10, 0, 20, 0},
+      Add, {4}, {/*scale=*/0.5, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 30, -10});
   test<ElementType::kSI8, ElementType::kF16>(
-      Add, {4}, {.scale = 1, .zero_point = 0}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/1, /*zero_point=*/0}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI16, ElementType::kF16>(
-      Add, {4}, {.scale = 2, .zero_point = 2}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/2, /*zero_point=*/2}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI32, ElementType::kF16>(
-      Add, {4}, {.scale = 0.5, .zero_point = -10}, {10, 0, 20, 0},
+      Add, {4}, {/*scale=*/0.5, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 30, -10});
   test<ElementType::kSI8, ElementType::kF32>(
-      Add, {4}, {.scale = 1, .zero_point = 0}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/1, /*zero_point=*/0}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI16, ElementType::kF32>(
-      Add, {4}, {.scale = 2, .zero_point = 2}, {10, 0, 20, 0}, {0, 0, 10, -10},
+      Add, {4}, {/*scale=*/2, /*zero_point=*/2}, {10, 0, 20, 0}, {0, 0, 10, -10},
       {10, 0, 30, -10});
   test<ElementType::kSI32, ElementType::kF32>(
-      Add, {4}, {.scale = 0.5, .zero_point = -10}, {10, 0, 20, 0},
+      Add, {4}, {/*scale=*/0.5, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 30, -10});
 }
 
@@ -156,39 +156,39 @@ TEST(ElementwiseBinary, Atan2) {
 
 TEST(ElementwiseBinary, Atan2Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Atan2, {4}, {.scale = 1e-1, .zero_point = 0}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Atan2, {4}, {.scale = 1e-1, .zero_point = 2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-1, /*zero_point=*/2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Atan2, {4}, {.scale = 1e-1, .zero_point = -2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-1, /*zero_point=*/-2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Atan2, {4}, {.scale = 1e-2, .zero_point = 0}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI16, ElementType::kF16>(
-      Atan2, {4}, {.scale = 1e-2, .zero_point = 2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-2, /*zero_point=*/2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Atan2, {4}, {.scale = 1e-3, .zero_point = -2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-3, /*zero_point=*/-2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Atan2, {4}, {.scale = 1e-2, .zero_point = 0}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI32, ElementType::kF16>(
-      Atan2, {4}, {.scale = 1e-2, .zero_point = 2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-2, /*zero_point=*/2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
   test<ElementType::kSI32, ElementType::kF32>(
-      Atan2, {4}, {.scale = 1e-3, .zero_point = -2}, {3, 0, 5, 3}, {1, 1, 4, 1},
+      Atan2, {4}, {/*scale=*/1e-3, /*zero_point=*/-2}, {3, 0, 5, 3}, {1, 1, 4, 1},
       {1.24904577239825442582f, 0, 0.89605538457134395617f,
        1.24904577239825442582f});
 }
@@ -210,31 +210,31 @@ TEST(ElementwiseBinary, Divide) {
 
 TEST(ElementwiseBinary, DivideQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Divide, {4}, {.scale = 1, .zero_point = 0}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI8, ElementType::kF16>(
-      Divide, {4}, {.scale = 1, .zero_point = 5}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/1, /*zero_point=*/5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI8, ElementType::kF32>(
-      Divide, {4}, {.scale = 1, .zero_point = -5}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/1, /*zero_point=*/-5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Divide, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI16, ElementType::kF16>(
-      Divide, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI16, ElementType::kF32>(
-      Divide, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Divide, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {111, 133.25, -83.25, 155.4});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Divide, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI32, ElementType::kF16>(
-      Divide, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Divide, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {11, 13.25, -8.25, 15.4});
   test<ElementType::kSI32, ElementType::kF32>(
-      Divide, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Divide, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {111, 133.25, -83.25, 155.4});
 }
 
@@ -257,31 +257,31 @@ TEST(ElementwiseBinary, Maximum) {
 
 TEST(ElementwiseBinary, MaximumQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Maximum, {4}, {.scale = 1, .zero_point = 0}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI8, ElementType::kF16>(
-      Maximum, {4}, {.scale = 1, .zero_point = 5}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/1, /*zero_point=*/5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI8, ElementType::kF32>(
-      Maximum, {4}, {.scale = 1, .zero_point = -5}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/1, /*zero_point=*/-5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Maximum, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI16, ElementType::kF16>(
-      Maximum, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI16, ElementType::kF32>(
-      Maximum, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Maximum, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {222, 533, 4, -5});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Maximum, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI32, ElementType::kF16>(
-      Maximum, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Maximum, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {22, 53, 4, -5});
   test<ElementType::kSI32, ElementType::kF32>(
-      Maximum, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Maximum, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {222, 533, 4, -5});
 }
 
@@ -304,31 +304,31 @@ TEST(ElementwiseBinary, Minimum) {
 
 TEST(ElementwiseBinary, MinimumQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Minimum, {4}, {.scale = 1, .zero_point = 0}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI8, ElementType::kF16>(
-      Minimum, {4}, {.scale = 1, .zero_point = 5}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/1, /*zero_point=*/5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI8, ElementType::kF32>(
-      Minimum, {4}, {.scale = 1, .zero_point = -5}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/1, /*zero_point=*/-5}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Minimum, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI16, ElementType::kF16>(
-      Minimum, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI16, ElementType::kF32>(
-      Minimum, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Minimum, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {2, 4, -333, -777});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Minimum, {4}, {.scale = 5e-1, .zero_point = 0}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/5e-1, /*zero_point=*/0}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI32, ElementType::kF16>(
-      Minimum, {4}, {.scale = 1e-1, .zero_point = 10}, {22, 53, -33, -77},
+      Minimum, {4}, {/*scale=*/1e-1, /*zero_point=*/10}, {22, 53, -33, -77},
       {2, 4, 4, -5}, {2, 4, -33, -77});
   test<ElementType::kSI32, ElementType::kF32>(
-      Minimum, {4}, {.scale = 5e-2, .zero_point = -10}, {222, 533, -333, -777},
+      Minimum, {4}, {/*scale=*/5e-2, /*zero_point=*/-10}, {222, 533, -333, -777},
       {2, 4, 4, -5}, {2, 4, -333, -777});
 }
 
@@ -351,31 +351,31 @@ TEST(ElementwiseBinary, Multiply) {
 
 TEST(ElementwiseBinary, MultiplyQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI8, ElementType::kF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI8, ElementType::kF32>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI16, ElementType::kF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI16, ElementType::kF32>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI32, ElementType::kF16>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
   test<ElementType::kSI32, ElementType::kF32>(
-      Multiply, {4}, {.scale = 1e-1, .zero_point = 0}, {1.1, 2.2, -3.3, -4.4},
+      Multiply, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {1.1, 2.2, -3.3, -4.4},
       {0.1, 1, 0.5, 2.5}, {0.11, 2.2, -1.7, -11});
 }
 
@@ -421,32 +421,32 @@ TEST(ElementwiseBinary, Remainder) {
 
 TEST(ElementwiseBinary, RemainderQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 0}, {7.1, -7.1, 7.1, -7.1},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {7.1, -7.1, 7.1, -7.1},
       {3, 3, -3, -3}, {1.1, -1.1, 1.1, -1.1});
   test<ElementType::kSI8, ElementType::kF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 0}, {7.1, -7.1, 7.1, -7.1},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {7.1, -7.1, 7.1, -7.1},
       {3, 3, -3, -3}, {1.1, -1.1, 1.1, -1.1});
   test<ElementType::kSI8, ElementType::kF32>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 0}, {7.1, -7.1, 7.1, -7.1},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {7.1, -7.1, 7.1, -7.1},
       {3, 3, -3, -3}, {1.1, -1.1, 1.1, -1.1});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 4}, {17, 18, 19, 20},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {17, 18, 19, 20},
       {3, 4, 5, 7}, {2, 2, 4, 6});
   test<ElementType::kSI16, ElementType::kF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 0}, {17, -17, 17, -17},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {17, -17, 17, -17},
       {3, 3, -3, -3}, {2, -2, 2, -2});
   test<ElementType::kSI16, ElementType::kF32>(
-      Remainder, {4}, {.scale = 1e-2, .zero_point = -10},
+      Remainder, {4}, {/*scale=*/1e-2, /*zero_point=*/-10},
       {17.1, -17.1, 17.1, -17.1}, {3, 3, -3, -3}, {2.1, -2.1, 2.1, -2.1});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 4}, {17, 18, 19, 20},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {17, 18, 19, 20},
       {3, 4, 5, 7}, {2, 2, 4, 6});
   test<ElementType::kSI32, ElementType::kF16>(
-      Remainder, {4}, {.scale = 1e-1, .zero_point = 0}, {17, -17, 17, -17},
+      Remainder, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {17, -17, 17, -17},
       {3, 3, -3, -3}, {2, -2, 2, -2});
   test<ElementType::kSI32, ElementType::kF32>(
-      Remainder, {4}, {.scale = 1e-2, .zero_point = -10},
+      Remainder, {4}, {/*scale=*/1e-2, /*zero_point=*/-10},
       {17.1, -17.1, 17.1, -17.1}, {3, 3, -3, -3}, {2.1, -2.1, 2.1, -2.1});
 }
 
@@ -491,31 +491,31 @@ TEST(ElementwiseBinary, Subtract) {
 
 TEST(ElementwiseBinary, SubtractQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Subtract, {4}, {.scale = 1, .zero_point = 0}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1, /*zero_point=*/0}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI8, ElementType::kF16>(
-      Subtract, {4}, {.scale = 1, .zero_point = 2}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1, /*zero_point=*/2}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI8, ElementType::kF32>(
-      Subtract, {4}, {.scale = 1, .zero_point = -10}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI16, ElementType::kBF16>(
-      Subtract, {4}, {.scale = 1e-1, .zero_point = 0}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI16, ElementType::kF16>(
-      Subtract, {4}, {.scale = 1e-1, .zero_point = 2}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-1, /*zero_point=*/2}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI16, ElementType::kF32>(
-      Subtract, {4}, {.scale = 1e-1, .zero_point = -10}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-1, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI32, ElementType::kBF16>(
-      Subtract, {4}, {.scale = 1e-3, .zero_point = 0}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-3, /*zero_point=*/0}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI32, ElementType::kF16>(
-      Subtract, {4}, {.scale = 1e-3, .zero_point = 2}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-3, /*zero_point=*/2}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
   test<ElementType::kSI32, ElementType::kF32>(
-      Subtract, {4}, {.scale = 1e-3, .zero_point = -10}, {10, 0, 20, 0},
+      Subtract, {4}, {/*scale=*/1e-3, /*zero_point=*/-10}, {10, 0, 20, 0},
       {0, 0, 10, -10}, {10, 0, 10, 10});
 }
 
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/elementwise_unary_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/elementwise_unary_test.cc
index 195157362d5..c88ec6a9a37 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/elementwise_unary_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/elementwise_unary_test.cc
@@ -93,16 +93,16 @@ TEST(ElementwiseUnary, Abs) {
 
 TEST(ElementwiseBinary, AbsQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Abs, {5}, {.scale = 1, .zero_point = 0}, {0, 1, -2, 3, -4},
+      Abs, {5}, {/*scale=*/1, /*zero_point=*/0}, {0, 1, -2, 3, -4},
       {0, 1, 2, 3, 4});
   test<ElementType::kSI8, ElementType::kF16>(
-      Abs, {5}, {.scale = 1e-1, .zero_point = 1}, {0, 1, -2, 3, -4},
+      Abs, {5}, {/*scale=*/1e-1, /*zero_point=*/1}, {0, 1, -2, 3, -4},
       {0, 1, 2, 3, 4});
   test<ElementType::kSI8, ElementType::kF32>(
-      Abs, {5}, {.scale = 1e-1, .zero_point = -1}, {0, 1, -2, 3, -4},
+      Abs, {5}, {/*scale=*/1e-1, /*zero_point=*/-1}, {0, 1, -2, 3, -4},
       {0, 1, 2, 3, 4});
   test<ElementType::kSI16, ElementType::kF32>(
-      Abs, {5}, {.scale = 1e-3, .zero_point = -1}, {0, 1, -2, 3, -4},
+      Abs, {5}, {/*scale=*/1e-3, /*zero_point=*/-1}, {0, 1, -2, 3, -4},
       {0, 1, 2, 3, 4});
 }
 
@@ -120,16 +120,16 @@ TEST(ElementwiseUnary, Cbrt) {
 
 TEST(ElementwiseUnary, CbrtQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Cbrt, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1, -2, 3},
+      Cbrt, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1, -2, 3},
       {0, 1, -1.25992104989487316476f, 1.44224957030740838232f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Cbrt, {4}, {.scale = 1e-1, .zero_point = -2}, {0, 1, -2, 3},
+      Cbrt, {4}, {/*scale=*/1e-1, /*zero_point=*/-2}, {0, 1, -2, 3},
       {0, 1, -1.25992104989487316476f, 1.44224957030740838232f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Cbrt, {4}, {.scale = 1e-1, .zero_point = 4}, {0, 1, -2, 3},
+      Cbrt, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {0, 1, -2, 3},
       {0, 1, -1.25992104989487316476f, 1.44224957030740838232f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Cbrt, {4}, {.scale = 1e-1, .zero_point = 4}, {0, 1, -2, 3},
+      Cbrt, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {0, 1, -2, 3},
       {0, 1, -1.25992104989487316476f, 1.44224957030740838232f});
 }
 
@@ -141,16 +141,16 @@ TEST(ElementwiseUnary, Ceil) {
 
 TEST(ElementwiseUnary, CeilQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Ceil, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1.1, -2.7, 3.5},
+      Ceil, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1.1, -2.7, 3.5},
       {0, 2, -2, 4});
   test<ElementType::kSI8, ElementType::kF16>(
-      Ceil, {4}, {.scale = 1e-1, .zero_point = 4}, {0, 1.1, -2.7, 3.5},
+      Ceil, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {0, 1.1, -2.7, 3.5},
       {0, 2, -2, 4});
   test<ElementType::kSI8, ElementType::kF32>(
-      Ceil, {4}, {.scale = 1e-1, .zero_point = -4}, {0, 1.1, -2.7, 3.5},
+      Ceil, {4}, {/*scale=*/1e-1, /*zero_point=*/-4}, {0, 1.1, -2.7, 3.5},
       {0, 2, -2, 4});
   test<ElementType::kSI16, ElementType::kF32>(
-      Ceil, {4}, {.scale = 1e-2, .zero_point = -4}, {0, 1.11, -2.77, 3.55},
+      Ceil, {4}, {/*scale=*/1e-2, /*zero_point=*/-4}, {0, 1.11, -2.77, 3.55},
       {0, 2, -2, 4});
 }
 
@@ -168,19 +168,19 @@ TEST(ElementwiseUnary, Cosine) {
 
 TEST(ElementwiseUnary, CosineQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Cosine, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1.1, -1.1, 2.3},
+      Cosine, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1.1, -1.1, 2.3},
       {1, 0.45359612142557738777f, 0.45359612142557738777f,
        -0.66627602127982419331f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Cosine, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1.1, -1.1, 2.3},
+      Cosine, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1.1, -1.1, 2.3},
       {1, 0.45359612142557738777f, 0.45359612142557738777f,
        -0.66627602127982419331f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Cosine, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1.1, -1.1, 2.3},
+      Cosine, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1.1, -1.1, 2.3},
       {1, 0.45359612142557738777f, 0.45359612142557738777f,
        -0.66627602127982419331f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Cosine, {4}, {.scale = 1e-4, .zero_point = 0}, {0, 1.1, -1.1, 2.3},
+      Cosine, {4}, {/*scale=*/1e-4, /*zero_point=*/0}, {0, 1.1, -1.1, 2.3},
       {1, 0.45359612142557738777f, 0.45359612142557738777f,
        -0.66627602127982419331f});
 }
@@ -208,19 +208,19 @@ TEST(ElementwiseUnary, Exponential) {
 
 TEST(ElementwiseUnary, ExponentialQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Exponential, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 0.5, 1, 1.5},
+      Exponential, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 0.5, 1, 1.5},
       {1, 1.64872127070012814684f, 2.71828182845904523536f,
        4.48168907033806482260f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Exponential, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 0.5, 1, 1.5},
+      Exponential, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 0.5, 1, 1.5},
       {1, 1.64872127070012814684f, 2.71828182845904523536f,
        4.48168907033806482260f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Exponential, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 0.5, 1, 1.5},
+      Exponential, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 0.5, 1, 1.5},
       {1, 1.64872127070012814684f, 2.71828182845904523536f,
        4.48168907033806482260f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Exponential, {4}, {.scale = 1e-2, .zero_point = 0}, {0, 0.5, 1, 1.5},
+      Exponential, {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {0, 0.5, 1, 1.5},
       {1, 1.64872127070012814684f, 2.71828182845904523536f,
        4.48168907033806482260f});
 }
@@ -239,22 +239,22 @@ TEST(ElementwiseUnary, ExponentialMinusOne) {
 
 TEST(ElementwiseUnary, ExponentialMinusOneQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      ExponentialMinusOne, {4}, {.scale = 1e-1, .zero_point = 0},
+      ExponentialMinusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, 0.5, 1, 1.5},
       {0, 0.64872127070012814684f, 1.71828182845904523536f,
        3.48168907033806482260f});
   test<ElementType::kSI8, ElementType::kF16>(
-      ExponentialMinusOne, {4}, {.scale = 1e-1, .zero_point = 0},
+      ExponentialMinusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, 0.5, 1, 1.5},
       {0, 0.64872127070012814684f, 1.71828182845904523536f,
        3.48168907033806482260f});
   test<ElementType::kSI8, ElementType::kF32>(
-      ExponentialMinusOne, {4}, {.scale = 1e-1, .zero_point = 0},
+      ExponentialMinusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, 0.5, 1, 1.5},
       {0, 0.64872127070012814684f, 1.71828182845904523536f,
        3.48168907033806482260f});
   test<ElementType::kSI16, ElementType::kF32>(
-      ExponentialMinusOne, {4}, {.scale = 1e-2, .zero_point = 0},
+      ExponentialMinusOne, {4}, {/*scale=*/1e-2, /*zero_point=*/0},
       {0, 0.5, 1, 1.5},
       {0, 0.64872127070012814684f, 1.71828182845904523536f,
        3.48168907033806482260f});
@@ -268,16 +268,16 @@ TEST(ElementwiseUnary, Floor) {
 
 TEST(ElementwiseUnary, FloorQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Floor, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1.1, -2.7, 3.5},
+      Floor, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1.1, -2.7, 3.5},
       {0, 1, -3, 3});
   test<ElementType::kSI8, ElementType::kF16>(
-      Floor, {4}, {.scale = 1e-1, .zero_point = 4}, {0, 1.1, -2.7, 3.5},
+      Floor, {4}, {/*scale=*/1e-1, /*zero_point=*/4}, {0, 1.1, -2.7, 3.5},
       {0, 1, -3, 3});
   test<ElementType::kSI8, ElementType::kF32>(
-      Floor, {4}, {.scale = 1e-1, .zero_point = -4}, {0, 1.1, -2.7, 3.5},
+      Floor, {4}, {/*scale=*/1e-1, /*zero_point=*/-4}, {0, 1.1, -2.7, 3.5},
       {0, 1, -3, 3});
   test<ElementType::kSI16, ElementType::kF32>(
-      Floor, {4}, {.scale = 1e-2, .zero_point = -4}, {0, 1.11, -2.77, 3.55},
+      Floor, {4}, {/*scale=*/1e-2, /*zero_point=*/-4}, {0, 1.11, -2.77, 3.55},
       {0, 1, -3, 3});
 }
 
@@ -295,19 +295,19 @@ TEST(ElementwiseUnary, Log) {
 
 TEST(ElementwiseUnary, LogQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Log, {4}, {.scale = 1e-1, .zero_point = -4}, {0.1, 0.5, 1, 1.5},
+      Log, {4}, {/*scale=*/1e-1, /*zero_point=*/-4}, {0.1, 0.5, 1, 1.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Log, {4}, {.scale = 1e-1, .zero_point = -4}, {0.1, 0.5, 1, 1.5},
+      Log, {4}, {/*scale=*/1e-1, /*zero_point=*/-4}, {0.1, 0.5, 1, 1.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Log, {4}, {.scale = 1e-1, .zero_point = -4}, {0.1, 0.5, 1, 1.5},
+      Log, {4}, {/*scale=*/1e-1, /*zero_point=*/-4}, {0.1, 0.5, 1, 1.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Log, {4}, {.scale = 1e-3, .zero_point = -4}, {0.1, 0.5, 1, 1.5},
+      Log, {4}, {/*scale=*/1e-3, /*zero_point=*/-4}, {0.1, 0.5, 1, 1.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
 }
@@ -326,19 +326,19 @@ TEST(ElementwiseUnary, LogPlusOne) {
 
 TEST(ElementwiseUnary, LogPlusOneQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      LogPlusOne, {4}, {.scale = 1e-1, .zero_point = 0}, {-0.9, -0.5, 0, 0.5},
+      LogPlusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-0.9, -0.5, 0, 0.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI8, ElementType::kF16>(
-      LogPlusOne, {4}, {.scale = 1e-1, .zero_point = 0}, {-0.9, -0.5, 0, 0.5},
+      LogPlusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-0.9, -0.5, 0, 0.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI8, ElementType::kF32>(
-      LogPlusOne, {4}, {.scale = 1e-1, .zero_point = 0}, {-0.9, -0.5, 0, 0.5},
+      LogPlusOne, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-0.9, -0.5, 0, 0.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
   test<ElementType::kSI16, ElementType::kF32>(
-      LogPlusOne, {4}, {.scale = 1e-4, .zero_point = 0}, {-0.9, -0.5, 0, 0.5},
+      LogPlusOne, {4}, {/*scale=*/1e-4, /*zero_point=*/0}, {-0.9, -0.5, 0, 0.5},
       {-2.30258509299404568401f, -0.69314718055994530941f, 0,
        0.40546510810816438197f});
 }
@@ -357,19 +357,19 @@ TEST(ElementwiseUnary, Logistic) {
 
 TEST(ElementwiseUnary, LogisticQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Logistic, {4}, {.scale = 1e-1, .zero_point = 0}, {-1, -0.5, 0, 0.5},
+      Logistic, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, -0.5, 0, 0.5},
       {0.26894142136999512074f, 0.37754066879814543536f, 0.5,
        0.62245933120185456464f});
   test<ElementType::kSI8, ElementType::kF16>(
-      Logistic, {4}, {.scale = 1e-1, .zero_point = 0}, {-1, -0.5, 0, 0.5},
+      Logistic, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, -0.5, 0, 0.5},
       {0.26894142136999512074f, 0.37754066879814543536f, 0.5,
        0.62245933120185456464f});
   test<ElementType::kSI8, ElementType::kF32>(
-      Logistic, {4}, {.scale = 1e-1, .zero_point = 0}, {-1, -0.5, 0, 0.5},
+      Logistic, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, -0.5, 0, 0.5},
       {0.26894142136999512074f, 0.37754066879814543536f, 0.5,
        0.62245933120185456464f});
   test<ElementType::kSI16, ElementType::kF32>(
-      Logistic, {4}, {.scale = 1e-3, .zero_point = 0}, {-1, -0.5, 0, 0.5},
+      Logistic, {4}, {/*scale=*/1e-3, /*zero_point=*/0}, {-1, -0.5, 0, 0.5},
       {0.26894142136999512074f, 0.37754066879814543536f, 0.5,
        0.62245933120185456464f});
 }
@@ -385,16 +385,16 @@ TEST(ElementwiseUnary, Negate) {
 
 TEST(ElementwiseBinary, NegateQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Negate, {5}, {.scale = 1, .zero_point = 0}, {0, 1, -2, 3, -4},
+      Negate, {5}, {/*scale=*/1, /*zero_point=*/0}, {0, 1, -2, 3, -4},
       {0, -1, 2, -3, 4});
   test<ElementType::kSI8, ElementType::kF16>(
-      Negate, {5}, {.scale = 1e-1, .zero_point = 1}, {0, 1, -2, 3, -4},
+      Negate, {5}, {/*scale=*/1e-1, /*zero_point=*/1}, {0, 1, -2, 3, -4},
       {0, -1, 2, -3, 4});
   test<ElementType::kSI8, ElementType::kF32>(
-      Negate, {5}, {.scale = 1e-1, .zero_point = -1}, {0, 1, -2, 3, -4},
+      Negate, {5}, {/*scale=*/1e-1, /*zero_point=*/-1}, {0, 1, -2, 3, -4},
       {0, -1, 2, -3, 4});
   test<ElementType::kSI16, ElementType::kF32>(
-      Negate, {5}, {.scale = 1e-3, .zero_point = -1}, {0, 1, -2, 3, -4},
+      Negate, {5}, {/*scale=*/1e-3, /*zero_point=*/-1}, {0, 1, -2, 3, -4},
       {0, -1, 2, -3, 4});
 }
 
@@ -427,16 +427,16 @@ TEST(ElementwiseUnary, RoundNearestAfz) {
 
 TEST(ElementwiseBinary, RoundNearestAfzQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      RoundNearestAfz, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestAfz, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-3.0, 0.0, 1.0, 1.0, 3.0});
   test<ElementType::kSI8, ElementType::kF16>(
-      RoundNearestAfz, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestAfz, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-3.0, 0.0, 1.0, 1.0, 3.0});
   test<ElementType::kSI8, ElementType::kF32>(
-      RoundNearestAfz, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestAfz, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-3.0, 0.0, 1.0, 1.0, 3.0});
   test<ElementType::kSI16, ElementType::kF32>(
-      RoundNearestAfz, {5}, {.scale = 1e-2, .zero_point = 0},
+      RoundNearestAfz, {5}, {/*scale=*/1e-2, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-3.0, 0.0, 1.0, 1.0, 3.0});
 }
 
@@ -451,16 +451,16 @@ TEST(ElementwiseUnary, RoundNearestEven) {
 
 TEST(ElementwiseBinary, RoundNearestEvenQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      RoundNearestEven, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestEven, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-2.0, 0.0, 0.0, 1.0, 2.0});
   test<ElementType::kSI8, ElementType::kF16>(
-      RoundNearestEven, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestEven, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-2.0, 0.0, 0.0, 1.0, 2.0});
   test<ElementType::kSI8, ElementType::kF32>(
-      RoundNearestEven, {5}, {.scale = 1e-1, .zero_point = 0},
+      RoundNearestEven, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-2.0, 0.0, 0.0, 1.0, 2.0});
   test<ElementType::kSI16, ElementType::kF32>(
-      RoundNearestEven, {5}, {.scale = 1e-2, .zero_point = 0},
+      RoundNearestEven, {5}, {/*scale=*/1e-2, /*zero_point=*/0},
       {-2.5, 0.4, 0.5, 0.6, 2.5}, {-2.0, 0.0, 0.0, 1.0, 2.0});
 }
 
@@ -475,7 +475,7 @@ TEST(ElementwiseUnary, Rsqrt) {
 
 TEST(ElementwiseUnary, RsqrtQuantized) {
   test<ElementType::kSI16, ElementType::kF32>(
-      Rsqrt, {4}, {.scale = 1e-3, .zero_point = 0}, {1.0, 4.0, 9.0, 25.0},
+      Rsqrt, {4}, {/*scale=*/1e-3, /*zero_point=*/0}, {1.0, 4.0, 9.0, 25.0},
       {1.0, 1.0 / 2.0, 1.0 / 3.0, 1.0 / 5.0});
 }
 
@@ -496,16 +496,16 @@ TEST(ElementwiseUnary, Sign) {
 
 TEST(ElementwiseUnary, SignQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Sign, {4}, {.scale = 1e-1, .zero_point = 0}, {-2.0, -0.0, +0.0, 2.0},
+      Sign, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-2.0, -0.0, +0.0, 2.0},
       {-1, 0, 0, 1});
   test<ElementType::kSI8, ElementType::kF16>(
-      Sign, {4}, {.scale = 1e-1, .zero_point = 0}, {-2.0, -0.0, +0.0, 2.0},
+      Sign, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-2.0, -0.0, +0.0, 2.0},
       {-1, 0, 0, 1});
   test<ElementType::kSI8, ElementType::kF32>(
-      Sign, {4}, {.scale = 1e-1, .zero_point = 0}, {-2.0, -0.0, +0.0, 2.0},
+      Sign, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {-2.0, -0.0, +0.0, 2.0},
       {-1, 0, 0, 1});
   test<ElementType::kSI16, ElementType::kF32>(
-      Sign, {4}, {.scale = 1e-2, .zero_point = 0}, {-2.0, -0.0, +0.0, 2.0},
+      Sign, {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {-2.0, -0.0, +0.0, 2.0},
       {-1, 0, 0, 1});
 }
 
@@ -520,16 +520,16 @@ TEST(ElementwiseUnary, Sine) {
 
 TEST(ElementwiseUnary, SineQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Sine, {5}, {.scale = 1e-1, .zero_point = 0},
+      Sine, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, M_PI_2, M_PI, 3 * M_PI_2, 2 * M_PI}, {0, 1, 0, -1, 0});
   test<ElementType::kSI8, ElementType::kF16>(
-      Sine, {5}, {.scale = 1e-1, .zero_point = 0},
+      Sine, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, M_PI_2, M_PI, 3 * M_PI_2, 2 * M_PI}, {0, 1, 0, -1, 0});
   test<ElementType::kSI8, ElementType::kF32>(
-      Sine, {5}, {.scale = 1e-1, .zero_point = 0},
+      Sine, {5}, {/*scale=*/1e-1, /*zero_point=*/0},
       {0, M_PI_2, M_PI, 3 * M_PI_2, 2 * M_PI}, {0, 1, 0, -1, 0});
   test<ElementType::kSI16, ElementType::kF32>(
-      Sine, {5}, {.scale = 1e-2, .zero_point = 0},
+      Sine, {5}, {/*scale=*/1e-2, /*zero_point=*/0},
       {0, M_PI_2, M_PI, 3 * M_PI_2, 2 * M_PI}, {0, 1, 0, -1, 0});
 }
 
@@ -541,13 +541,13 @@ TEST(ElementwiseUnary, Sqrt) {
 
 TEST(ElementwiseUnary, SqrtQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Sqrt, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1, 4, 9}, {0, 1, 2, 3});
+      Sqrt, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1, 4, 9}, {0, 1, 2, 3});
   test<ElementType::kSI8, ElementType::kF16>(
-      Sqrt, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1, 4, 9}, {0, 1, 2, 3});
+      Sqrt, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1, 4, 9}, {0, 1, 2, 3});
   test<ElementType::kSI8, ElementType::kF32>(
-      Sqrt, {4}, {.scale = 1e-1, .zero_point = 0}, {0, 1, 4, 9}, {0, 1, 2, 3});
+      Sqrt, {4}, {/*scale=*/1e-1, /*zero_point=*/0}, {0, 1, 4, 9}, {0, 1, 2, 3});
   test<ElementType::kSI16, ElementType::kF32>(
-      Sqrt, {4}, {.scale = 1e-2, .zero_point = 0}, {0, 1, 4, 9}, {0, 1, 2, 3});
+      Sqrt, {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {0, 1, 4, 9}, {0, 1, 2, 3});
 }
 
 TEST(ElementwiseUnary, Tanh) {
@@ -561,16 +561,16 @@ TEST(ElementwiseUnary, Tanh) {
 
 TEST(ElementwiseUnary, TanhQuantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      Tanh, {3}, {.scale = 1e-1, .zero_point = 0}, {-1, 0, 1},
+      Tanh, {3}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, 0, 1},
       {-0.76159416, 0.0, 0.76159416});
   test<ElementType::kSI8, ElementType::kF16>(
-      Tanh, {3}, {.scale = 1e-1, .zero_point = 0}, {-1, 0, 1},
+      Tanh, {3}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, 0, 1},
       {-0.76159416, 0.0, 0.76159416});
   test<ElementType::kSI8, ElementType::kF32>(
-      Tanh, {3}, {.scale = 1e-1, .zero_point = 0}, {-1, 0, 1},
+      Tanh, {3}, {/*scale=*/1e-1, /*zero_point=*/0}, {-1, 0, 1},
       {-0.76159416, 0.0, 0.76159416});
   test<ElementType::kSI16, ElementType::kF32>(
-      Tanh, {3}, {.scale = 1e-2, .zero_point = 0}, {-1, 0, 1},
+      Tanh, {3}, {/*scale=*/1e-2, /*zero_point=*/0}, {-1, 0, 1},
       {-0.76159416, 0.0, 0.76159416});
 }
 
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/iota_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/iota_test.cc
index 7fe34cdc01d..fda533df81d 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/iota_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/iota_test.cc
@@ -101,60 +101,60 @@ TEST(Iota, Unquantized) {
 
 TEST(Iota, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/0.1, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 1e-3, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-3, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 1e-3, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-3, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 1e-2, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-2, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 1e-3, .zero_point = 0}, {4, 5}, 0,
+      {/*scale=*/1e-3, /*zero_point=*/0}, {4, 5}, 0,
       {0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 1e-3, .zero_point = 0}, {4, 5}, 1,
+      {/*scale=*/1e-3, /*zero_point=*/0}, {4, 5}, 1,
       {0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4});
 }
 
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/is_finite_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/is_finite_test.cc
index 5c097287233..4442ab3577d 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/is_finite_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/is_finite_test.cc
@@ -74,41 +74,41 @@ TEST(IsFinite, Unquantized) {
 
 TEST(IsFinite, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI8, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI16, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI32, ElementType::kF16>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {7},
+      {/*scale=*/0.1, /*zero_point=*/0}, {7},
       {+NAN, -NAN, -INFINITY, +INFINITY, -1, 0, 1},
       {false, false, false, false, true, true, true});
 }
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/select_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/select_test.cc
index d82accf8aba..76943a627e6 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/select_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/select_test.cc
@@ -118,53 +118,53 @@ TEST(Select, Unquantized) {
 
 TEST(Select, Quantized) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI8, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI8, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                              {2}, {false}, {1, 2}, {-1, -2},
                                              {-1, -2});
   test<ElementType::kSI8, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI8, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI8, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {true, false}, {1, 2},
                                               {-1, -2}, {1, -2});
-  test<ElementType::kSI8, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI8, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                              {2}, {true, false}, {1, 2},
                                              {-1, -2}, {1, -2});
-  test<ElementType::kSI8, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI8, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                              {2}, {true, false}, {1, 2},
                                              {-1, -2}, {1, -2});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI16, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI16, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {false}, {1, 2}, {-1, -2},
                                               {-1, -2});
   test<ElementType::kSI16, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI16, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI16, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                                {2}, {true, false}, {1, 2},
                                                {-1, -2}, {1, -2});
-  test<ElementType::kSI16, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI16, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {true, false}, {1, 2},
                                               {-1, -2}, {1, -2});
-  test<ElementType::kSI16, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI16, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {true, false}, {1, 2},
                                               {-1, -2}, {1, -2});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI32, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI32, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {false}, {1, 2}, {-1, -2},
                                               {-1, -2});
   test<ElementType::kSI32, ElementType::kF32>(
-      {.scale = 0.1, .zero_point = 0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
-  test<ElementType::kSI32, ElementType::kBF16>({.scale = 0.1, .zero_point = 0},
+      {/*scale=*/0.1, /*zero_point=*/0}, {2}, {true}, {1, 2}, {-1, -2}, {1, 2});
+  test<ElementType::kSI32, ElementType::kBF16>({/*scale=*/0.1, /*zero_point=*/0},
                                                {2}, {true, false}, {1, 2},
                                                {-1, -2}, {1, -2});
-  test<ElementType::kSI32, ElementType::kF16>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI32, ElementType::kF16>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {true, false}, {1, 2},
                                               {-1, -2}, {1, -2});
-  test<ElementType::kSI32, ElementType::kF32>({.scale = 0.1, .zero_point = 0},
+  test<ElementType::kSI32, ElementType::kF32>({/*scale=*/0.1, /*zero_point=*/0},
                                               {2}, {true, false}, {1, 2},
                                               {-1, -2}, {1, -2});
 }
diff --git a/tensorflow/lite/experimental/shlo/legacy/test/uniform_dequantize_quantize_test.cc b/tensorflow/lite/experimental/shlo/legacy/test/uniform_dequantize_quantize_test.cc
index 9cb5d288ced..17d87d063cd 100644
--- a/tensorflow/lite/experimental/shlo/legacy/test/uniform_dequantize_quantize_test.cc
+++ b/tensorflow/lite/experimental/shlo/legacy/test/uniform_dequantize_quantize_test.cc
@@ -52,58 +52,58 @@ void test(std::initializer_list<DimensionSize>&& shape,
 
 TEST(QuantizeDequantize, All) {
   test<ElementType::kSI8, ElementType::kBF16>(
-      {4}, {.scale = 1, .zero_point = 0}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/0}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI8, ElementType::kBF16>(
-      {4}, {.scale = 1, .zero_point = 0}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/0}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI8, ElementType::kBF16>(
-      {4}, {.scale = 1e-1, .zero_point = -5}, {-2.2, -1.1, 0, 1.1, 2.2});
-  test<ElementType::kSI8, ElementType::kF16>({4}, {.scale = 1, .zero_point = 5},
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-5}, {-2.2, -1.1, 0, 1.1, 2.2});
+  test<ElementType::kSI8, ElementType::kF16>({4}, {/*scale=*/1, /*zero_point=*/5},
                                              {-2, -1, 0, 1, 2});
   test<ElementType::kSI8, ElementType::kF16>(
-      {4}, {.scale = 1e-1, .zero_point = -10}, {-2.2, -1.1, 0, 1.1, 2.2});
-  test<ElementType::kSI8, ElementType::kF32>({4}, {.scale = 1, .zero_point = 5},
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-10}, {-2.2, -1.1, 0, 1.1, 2.2});
+  test<ElementType::kSI8, ElementType::kF32>({4}, {/*scale=*/1, /*zero_point=*/5},
                                              {-2, -1, 0, 1, 2});
   test<ElementType::kSI8, ElementType::kF32>(
-      {4}, {.scale = 1e-1, .zero_point = +10}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/+10}, {-2.2, -1.1, 0, 1.1, 2.2});
 
   test<ElementType::kSI16, ElementType::kBF16>(
-      {4}, {.scale = 1, .zero_point = 0}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/0}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI16, ElementType::kBF16>(
-      {4}, {.scale = 1e-1, .zero_point = 5}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/5}, {-2.2, -1.1, 0, 1.1, 2.2});
   test<ElementType::kSI16, ElementType::kBF16>(
-      {4}, {.scale = 1e-2, .zero_point = -5}, {-2.22, -1.11, 0, 1.11, 2.22});
+      {4}, {/*scale=*/1e-2, /*zero_point=*/-5}, {-2.22, -1.11, 0, 1.11, 2.22});
   test<ElementType::kSI16, ElementType::kF16>(
-      {4}, {.scale = 1, .zero_point = 0}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/0}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI16, ElementType::kF16>(
-      {4}, {.scale = 1e-1, .zero_point = -10}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-10}, {-2.2, -1.1, 0, 1.1, 2.2});
   test<ElementType::kSI16, ElementType::kF16>(
-      {4}, {.scale = 1e-2, .zero_point = 10}, {-2.22, -1.11, 0, 1.11, 2.22});
+      {4}, {/*scale=*/1e-2, /*zero_point=*/10}, {-2.22, -1.11, 0, 1.11, 2.22});
 
   test<ElementType::kSI32, ElementType::kBF16>(
-      {4}, {.scale = 1, .zero_point = +7}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/+7}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI32, ElementType::kBF16>(
-      {4}, {.scale = 1e-1, .zero_point = -7}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-7}, {-2.2, -1.1, 0, 1.1, 2.2});
   test<ElementType::kSI32, ElementType::kBF16>(
-      {4}, {.scale = 1e-2, .zero_point = 0}, {-2.22, -1.11, 0, 1.11, 2.22});
+      {4}, {/*scale=*/1e-2, /*zero_point=*/0}, {-2.22, -1.11, 0, 1.11, 2.22});
   test<ElementType::kSI32, ElementType::kBF16>(
-      {4}, {.scale = 1e-3, .zero_point = 0}, {-2.222, -1.111, 0, 1.111, 2.222});
+      {4}, {/*scale=*/1e-3, /*zero_point=*/0}, {-2.222, -1.111, 0, 1.111, 2.222});
   test<ElementType::kSI32, ElementType::kF16>(
-      {4}, {.scale = 1, .zero_point = +7}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/+7}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI32, ElementType::kF16>(
-      {4}, {.scale = 1e-1, .zero_point = -7}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-7}, {-2.2, -1.1, 0, 1.1, 2.2});
   test<ElementType::kSI32, ElementType::kF16>(
-      {4}, {.scale = 1e-2, .zero_point = 10}, {-2.22, -1.11, 0, 1.11, 2.22});
+      {4}, {/*scale=*/1e-2, /*zero_point=*/10}, {-2.22, -1.11, 0, 1.11, 2.22});
   test<ElementType::kSI32, ElementType::kF16>(
-      {4}, {.scale = 1e-3, .zero_point = -0},
+      {4}, {/*scale=*/1e-3, /*zero_point=*/-0},
       {-2.222, -1.111, 0, 1.111, 2.222});
   test<ElementType::kSI32, ElementType::kF32>(
-      {4}, {.scale = 1, .zero_point = +7}, {-2, -1, 0, 1, 2});
+      {4}, {/*scale=*/1, /*zero_point=*/+7}, {-2, -1, 0, 1, 2});
   test<ElementType::kSI32, ElementType::kF32>(
-      {4}, {.scale = 1e-1, .zero_point = -7}, {-2.2, -1.1, 0, 1.1, 2.2});
+      {4}, {/*scale=*/1e-1, /*zero_point=*/-7}, {-2.2, -1.1, 0, 1.1, 2.2});
   test<ElementType::kSI32, ElementType::kF32>(
-      {4}, {.scale = 1e-2, .zero_point = 10}, {-2.22, -1.11, 0, 1.11, 2.22});
+      {4}, {/*scale=*/1e-2, /*zero_point=*/10}, {-2.22, -1.11, 0, 1.11, 2.22});
   test<ElementType::kSI32, ElementType::kF32>(
-      {4}, {.scale = 1e-3, .zero_point = -0},
+      {4}, {/*scale=*/1e-3, /*zero_point=*/-0},
       {-2.222, -1.111, 0, 1.111, 2.222});
 }
 
diff --git a/tensorflow/lite/experimental/shlo/ops/abs_test.cc b/tensorflow/lite/experimental/shlo/ops/abs_test.cc
index b50587479b2..2da5b2e8aae 100644
--- a/tensorflow/lite/experimental/shlo/ops/abs_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/abs_test.cc
@@ -70,11 +70,11 @@ TYPED_TEST(AbsTest, ArithmeticTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), abs_ref);
@@ -103,13 +103,13 @@ TYPED_TEST(QuantizedAbsTest, QuantizedPerTensor) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/and_test.cc b/tensorflow/lite/experimental/shlo/ops/and_test.cc
index 7b22155f743..8302a163468 100644
--- a/tensorflow/lite/experimental/shlo/ops/and_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/and_test.cc
@@ -84,14 +84,14 @@ TYPED_TEST(AndTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(),
diff --git a/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test.cc b/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test.cc
index 04052c4e94f..af987f819be 100644
--- a/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test.cc
@@ -51,14 +51,14 @@ TYPED_TEST(EvaluateNoQuantizationTest, ArithmeticTensorsWithTestOp) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), TestOp());
@@ -90,27 +90,27 @@ TYPED_TEST(DequantizeOpQuantizePerTensor, QuantizedPerTensorWithTestOp) {
   const StorageT rhs_zero_point = static_cast<StorageT>(5);
   const ExpressedT output_scale = static_cast<ExpressedT>(1.5);
   const StorageT output_zero_point = static_cast<StorageT>(3);
-  Tensor lhs_tensor{.type =
+  Tensor lhs_tensor{/*type=*/
                         QuantizedPerTensorTensorType{
-                            .shape = shape,
-                            .element_type = QuantizedElementTypePerTensor(
+                            /*shape=*/shape,
+                            /*element_type=*/QuantizedElementTypePerTensor(
                                 TypeParam::kStorage, lhs_zero_point,
                                 TypeParam::kExpressed, lhs_scale)},
-                    .data = lhs_data.data()};
-  Tensor rhs_tensor{.type =
+                    /*data=*/lhs_data.data()};
+  Tensor rhs_tensor{/*type=*/
                         QuantizedPerTensorTensorType{
-                            .shape = shape,
-                            .element_type = QuantizedElementTypePerTensor(
+                            /*shape=*/shape,
+                            /*element_type=*/QuantizedElementTypePerTensor(
                                 TypeParam::kStorage, rhs_zero_point,
                                 TypeParam::kExpressed, rhs_scale)},
-                    .data = rhs_data.data()};
-  Tensor output_tensor{.type =
+                    /*data=*/rhs_data.data()};
+  Tensor output_tensor{/*type=*/
                            QuantizedPerTensorTensorType{
-                               .shape = shape,
-                               .element_type = QuantizedElementTypePerTensor(
+                               /*shape=*/shape,
+                               /*element_type=*/QuantizedElementTypePerTensor(
                                    TypeParam::kStorage, output_zero_point,
                                    TypeParam::kExpressed, output_scale)},
-                       .data = output_data.data()};
+                       /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test_util.h b/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test_util.h
index 252678588d1..6b4de44fa60 100644
--- a/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test_util.h
+++ b/tensorflow/lite/experimental/shlo/ops/binary_elementwise_test_util.h
@@ -73,18 +73,18 @@ class BinaryElementwiseOpShapePropagationTest : public ::testing::Test {
 
   Op op_ = Create(SupportedOpAttributes<Op>::Get());
   Tensor lhs_tensor_ = {
-      .type = TensorType{.shape = Shape({2, 3, 4}),
-                         .element_type = SupportedOpDataType<Op>::kStorageType},
-      .data = nullptr};
+      /*type=*/TensorType{/*shape=*/Shape({2, 3, 4}),
+                         /*element_type=*/SupportedOpDataType<Op>::kStorageType},
+      /*data=*/nullptr};
   Tensor rhs_tensor_ = {
-      .type = TensorType{.shape = Shape({2, 3, 4}),
-                         .element_type = SupportedOpDataType<Op>::kStorageType},
-      .data = nullptr};
+      /*type=*/TensorType{/*shape=*/Shape({2, 3, 4}),
+                         /*element_type=*/SupportedOpDataType<Op>::kStorageType},
+      /*data=*/nullptr};
   Tensor output_tensor_ = {
-      .type = TensorType{.shape = Shape(),
-                         .element_type =
+      /*type=*/TensorType{/*shape=*/Shape(),
+                         /*element_type=*/
                              SupportedOpOutputDataType<Op>::kStorageType},
-      .data = nullptr};
+      /*data=*/nullptr};
 };
 
 TYPED_TEST_SUITE_P(BinaryElementwiseOpShapePropagationTest);
@@ -151,12 +151,12 @@ TYPED_TEST_P(BinaryElementwiseSameBaselineElementTypeConstraintTest,
   using RhsTypeDesc = std::tuple_element_t<2, TypeParam>;
   using ResultTypeDesc = std::tuple_element_t<3, TypeParam>;
   const Shape shape({2, 3, 4});
-  Tensor lhs_tensor{.type = TensorTypeFor(LhsTypeDesc{}, shape),
-                    .data = nullptr};
-  Tensor rhs_tensor{.type = TensorTypeFor(RhsTypeDesc{}, shape),
-                    .data = nullptr};
-  Tensor output_tensor{.type = TensorTypeFor(ResultTypeDesc{}, shape),
-                       .data = nullptr};
+  Tensor lhs_tensor{/*type=*/TensorTypeFor(LhsTypeDesc{}, shape),
+                    /*data=*/nullptr};
+  Tensor rhs_tensor{/*type=*/TensorTypeFor(RhsTypeDesc{}, shape),
+                    /*data=*/nullptr};
+  Tensor output_tensor{/*type=*/TensorTypeFor(ResultTypeDesc{}, shape),
+                       /*data=*/nullptr};
   auto op = Create(typename Op::Attributes{});
   const absl::Status status =
       Prepare(op, lhs_tensor, rhs_tensor, output_tensor);
@@ -181,8 +181,8 @@ TYPED_TEST_SUITE_P(BinaryElementwiseUnsupportedTypeTest);
 TYPED_TEST_P(BinaryElementwiseUnsupportedTypeTest, PrepareRaisesAnError) {
   using Op = std::tuple_element_t<0, TypeParam>;
   using TypeDesc = std::tuple_element_t<1, TypeParam>;
-  Tensor input_tensor{.type = TensorTypeFor(TypeDesc{}, Shape({2, 3, 4})),
-                      .data = nullptr};
+  Tensor input_tensor{/*type=*/TensorTypeFor(TypeDesc{}, Shape({2, 3, 4})),
+                      /*data=*/nullptr};
   Tensor output_tensor = input_tensor;
   auto op = Create(typename Op::Attributes{});
   const absl::Status status =
diff --git a/tensorflow/lite/experimental/shlo/ops/cbrt_test.cc b/tensorflow/lite/experimental/shlo/ops/cbrt_test.cc
index bbb437e0648..750886d86fe 100644
--- a/tensorflow/lite/experimental/shlo/ops/cbrt_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/cbrt_test.cc
@@ -86,11 +86,11 @@ TYPED_TEST(CbrtTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), cbrt_ref);
@@ -119,13 +119,13 @@ TYPED_TEST(QuantizedCbrtTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/ceil_test.cc b/tensorflow/lite/experimental/shlo/ops/ceil_test.cc
index e0074922c7e..ceb75fbfe28 100644
--- a/tensorflow/lite/experimental/shlo/ops/ceil_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/ceil_test.cc
@@ -86,11 +86,11 @@ TYPED_TEST(CeilTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), ceil_ref);
@@ -119,13 +119,13 @@ TYPED_TEST(QuantizedCeilTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/compare.cc b/tensorflow/lite/experimental/shlo/ops/compare.cc
index 651be32f127..bc6725ca423 100644
--- a/tensorflow/lite/experimental/shlo/ops/compare.cc
+++ b/tensorflow/lite/experimental/shlo/ops/compare.cc
@@ -59,7 +59,7 @@ void DequantizeCompare(F&& func, const Tensor& lhs, const Tensor& rhs,
 }  // namespace
 
 CompareOp Create(CompareOp::Attributes attributes) {
-  return {.attributes = attributes};
+  return {/*attributes=*/attributes};
 }
 
 absl::Status Prepare(CompareOp& op, const Tensor& lhs, const Tensor& rhs,
diff --git a/tensorflow/lite/experimental/shlo/ops/compare_test.cc b/tensorflow/lite/experimental/shlo/ops/compare_test.cc
index 8580ae3e365..62d19db8f8f 100644
--- a/tensorflow/lite/experimental/shlo/ops/compare_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/compare_test.cc
@@ -85,7 +85,7 @@ const char* ToString(CompareOp::ComparisonDirection comparison_direction) {
 template <>
 struct SupportedOpAttributes<CompareOp> {
   static CompareOp::Attributes Get() {
-    return {.comparison_direction = CompareOp::ComparisonDirection::kEq};
+    return {/*comparison_direction=*/CompareOp::ComparisonDirection::kEq};
   }
 };
 
@@ -130,12 +130,12 @@ TYPED_TEST(CompareSameBaselineElementTypeConstraintTest,
   using LhsTypeDesc = std::tuple_element_t<1, TypeParam>;
   using RhsTypeDesc = std::tuple_element_t<2, TypeParam>;
   const Shape shape({2, 3, 4});
-  Tensor lhs_tensor{.type = TensorTypeFor(LhsTypeDesc{}, shape),
-                    .data = nullptr};
-  Tensor rhs_tensor{.type = TensorTypeFor(RhsTypeDesc{}, shape),
-                    .data = nullptr};
-  Tensor output_tensor{.type = TensorTypeFor(TestParam<DataType::kI1>{}, shape),
-                       .data = nullptr};
+  Tensor lhs_tensor{/*type=*/TensorTypeFor(LhsTypeDesc{}, shape),
+                    /*data=*/nullptr};
+  Tensor rhs_tensor{/*type=*/TensorTypeFor(RhsTypeDesc{}, shape),
+                    /*data=*/nullptr};
+  Tensor output_tensor{/*type=*/TensorTypeFor(TestParam<DataType::kI1>{}, shape),
+                       /*data=*/nullptr};
   auto op = Create(typename Op::Attributes{});
   const absl::Status status =
       Prepare(op, lhs_tensor, rhs_tensor, output_tensor);
@@ -171,14 +171,14 @@ TYPED_TEST(CompareTest, SupportedTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = DataType::kI1},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/DataType::kI1},
+      /*data=*/output_data.data()};
 
   const CompareOp::ComparisonDirection comparison_direction =
       static_cast<CompareOp::ComparisonDirection>(absl::Uniform(bit_gen, 0, 6));
@@ -189,7 +189,7 @@ TYPED_TEST(CompareTest, SupportedTestTypesTensorsWork) {
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), compare_ref);
 
   auto op = Create(
-      CompareOp::Attributes{.comparison_direction = comparison_direction});
+      CompareOp::Attributes{/*comparison_direction=*/comparison_direction});
   ASSERT_OK(Prepare(op, lhs_tensor, rhs_tensor, output_tensor));
   ASSERT_OK(Evaluate(op, lhs_tensor, rhs_tensor, output_tensor));
   EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));
@@ -218,16 +218,16 @@ TYPED_TEST(QuantizedCompareTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = DataType::kI1},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/DataType::kI1},
+      /*data=*/output_data.data()};
 
   const CompareOp::ComparisonDirection comparison_direction =
       CompareOp::ComparisonDirection::kEq;
@@ -245,7 +245,7 @@ TYPED_TEST(QuantizedCompareTest, PerTensorWorks) {
       });
 
   auto op = Create(
-      CompareOp::Attributes{.comparison_direction = comparison_direction});
+      CompareOp::Attributes{/*comparison_direction=*/comparison_direction});
   ASSERT_OK(Prepare(op, lhs_tensor, rhs_tensor, output_tensor));
   ASSERT_OK(Evaluate(op, lhs_tensor, rhs_tensor, output_tensor));
   EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data))
diff --git a/tensorflow/lite/experimental/shlo/ops/cosine_test.cc b/tensorflow/lite/experimental/shlo/ops/cosine_test.cc
index 99b5e03c43b..6878fadf239 100644
--- a/tensorflow/lite/experimental/shlo/ops/cosine_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/cosine_test.cc
@@ -87,11 +87,11 @@ TYPED_TEST(CosineTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), cosine_ref);
@@ -120,13 +120,13 @@ TYPED_TEST(QuantizedCosineTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/count_leading_zeros_test.cc b/tensorflow/lite/experimental/shlo/ops/count_leading_zeros_test.cc
index 47796b9b371..7389ccf6ddc 100644
--- a/tensorflow/lite/experimental/shlo/ops/count_leading_zeros_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/count_leading_zeros_test.cc
@@ -110,11 +110,11 @@ TYPED_TEST(CountLeadingZerosTest, IntTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), count_leading_zeros_ref);
diff --git a/tensorflow/lite/experimental/shlo/ops/divide_test.cc b/tensorflow/lite/experimental/shlo/ops/divide_test.cc
index fe6fa64dc21..0c414ee438a 100644
--- a/tensorflow/lite/experimental/shlo/ops/divide_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/divide_test.cc
@@ -77,14 +77,14 @@ TYPED_TEST(DivideTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), Divide());
@@ -116,17 +116,17 @@ TYPED_TEST(QuantizedDivideTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/exponential_minus_one_test.cc b/tensorflow/lite/experimental/shlo/ops/exponential_minus_one_test.cc
index 24cff531bdb..d8f0dab9763 100644
--- a/tensorflow/lite/experimental/shlo/ops/exponential_minus_one_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/exponential_minus_one_test.cc
@@ -89,11 +89,11 @@ TYPED_TEST(ExponentialMinusOneTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(),
@@ -124,13 +124,13 @@ TYPED_TEST(QuantizedExponentialMinusOneTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/exponential_test.cc b/tensorflow/lite/experimental/shlo/ops/exponential_test.cc
index eaa94571da7..4a02deee5d9 100644
--- a/tensorflow/lite/experimental/shlo/ops/exponential_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/exponential_test.cc
@@ -88,11 +88,11 @@ TYPED_TEST(ExponentialTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), exponential_ref);
@@ -121,13 +121,13 @@ TYPED_TEST(QuantizedExponentialTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/floor_test.cc b/tensorflow/lite/experimental/shlo/ops/floor_test.cc
index 5c0afaa1ced..1c1f5b8116c 100644
--- a/tensorflow/lite/experimental/shlo/ops/floor_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/floor_test.cc
@@ -87,11 +87,11 @@ TYPED_TEST(FloorTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), floor_ref);
@@ -120,13 +120,13 @@ TYPED_TEST(QuantizedFloorTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/is_finite_bench.cc b/tensorflow/lite/experimental/shlo/ops/is_finite_bench.cc
index e2a327e1a2e..63965420dea 100644
--- a/tensorflow/lite/experimental/shlo/ops/is_finite_bench.cc
+++ b/tensorflow/lite/experimental/shlo/ops/is_finite_bench.cc
@@ -31,8 +31,8 @@ void BM_IsFinite(benchmark::State& state, DimensionSize num_elements,
                  const Tensor& operand) {
   IsFiniteOp op = Create(IsFiniteOp::Attributes{});
 
-  Tensor result = Tensor{.type = TensorType{.shape = Shape{{num_elements}},
-                                            .element_type = DataType::kI1}};
+  Tensor result = Tensor{/*type=*/TensorType{/*shape=*/Shape{{num_elements}},
+                                            /*element_type=*/DataType::kI1}};
   ABSL_CHECK_OK(Prepare(op, operand, result));
 
   std::vector<std::byte> result_values(result.SizeInBytes());
diff --git a/tensorflow/lite/experimental/shlo/ops/is_finite_test.cc b/tensorflow/lite/experimental/shlo/ops/is_finite_test.cc
index f9ce80db4ce..1ee6d81f5f7 100644
--- a/tensorflow/lite/experimental/shlo/ops/is_finite_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/is_finite_test.cc
@@ -46,7 +46,7 @@ TEST_P(IsFiniteTest, IsFinite) {
   const auto& params = GetParam();
 
   IsFiniteOp op = Create(IsFiniteOp::Attributes{});
-  Tensor result{.type = params.expected.tensor().type};
+  Tensor result{/*type=*/params.expected.tensor().type};
 
   ASSERT_OK(Prepare(op, params.operand.tensor(), result));
 
@@ -82,9 +82,9 @@ INSTANTIATE_TEST_SUITE_P(
 INSTANTIATE_TEST_SUITE_P(
     Quantized, IsFiniteTest,
     ::testing::Values(Params{
-        .operand = TensorWithData::Create<DataType::kSI16, DataType::kF32>(
+        /*operand=*/TensorWithData::Create<DataType::kSI16, DataType::kF32>(
             Shape{{7}}, {0.0f, -1.0f, 0.0f, 1.0f, 1.0f, 0.0f, 1.0f}, 0.1f, 0),
-        .expected = TensorWithData::Create<DataType::kI1>(
+        /*expected=*/TensorWithData::Create<DataType::kI1>(
             Shape{{7}}, {true, true, true, true, true, true, true})}));
 }  // namespace
 }  // namespace shlo_ref
diff --git a/tensorflow/lite/experimental/shlo/ops/log_plus_one_test.cc b/tensorflow/lite/experimental/shlo/ops/log_plus_one_test.cc
index d9eb32294e0..2a46c9bf2f5 100644
--- a/tensorflow/lite/experimental/shlo/ops/log_plus_one_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/log_plus_one_test.cc
@@ -89,11 +89,11 @@ TYPED_TEST(LogPlusOneTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), log_plus_one_ref);
@@ -123,13 +123,13 @@ TYPED_TEST(QuantizedLogPlusOneTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/log_test.cc b/tensorflow/lite/experimental/shlo/ops/log_test.cc
index d83bec53f6d..5717803d48e 100644
--- a/tensorflow/lite/experimental/shlo/ops/log_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/log_test.cc
@@ -87,11 +87,11 @@ TYPED_TEST(LogTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), log_ref);
@@ -121,13 +121,13 @@ TYPED_TEST(QuantizedLogTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/logistic_test.cc b/tensorflow/lite/experimental/shlo/ops/logistic_test.cc
index 0b8022589f4..1b7a7b401e4 100644
--- a/tensorflow/lite/experimental/shlo/ops/logistic_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/logistic_test.cc
@@ -88,11 +88,11 @@ TYPED_TEST(LogisticTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), logistic_ref);
@@ -121,13 +121,13 @@ TYPED_TEST(QuantizedLogisticTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/maximum_test.cc b/tensorflow/lite/experimental/shlo/ops/maximum_test.cc
index 331b229b6de..80a9fb320a4 100644
--- a/tensorflow/lite/experimental/shlo/ops/maximum_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/maximum_test.cc
@@ -81,14 +81,14 @@ TYPED_TEST(MaximumTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), Maximum());
@@ -120,17 +120,17 @@ TYPED_TEST(QuantizedMaximumTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/minimum_test.cc b/tensorflow/lite/experimental/shlo/ops/minimum_test.cc
index 6f1a1b32071..f57f45d6fd4 100644
--- a/tensorflow/lite/experimental/shlo/ops/minimum_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/minimum_test.cc
@@ -81,14 +81,14 @@ TYPED_TEST(MinimumTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), Minimum());
@@ -120,17 +120,17 @@ TYPED_TEST(QuantizedMinimumTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/multiply_test.cc b/tensorflow/lite/experimental/shlo/ops/multiply_test.cc
index 2716f4910d8..7992072e0be 100644
--- a/tensorflow/lite/experimental/shlo/ops/multiply_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/multiply_test.cc
@@ -88,14 +88,14 @@ TYPED_TEST(MultiplyTest, ArithmeticTestTypesTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(),
@@ -128,17 +128,17 @@ TYPED_TEST(QuantizedMultiplyTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/negate_test.cc b/tensorflow/lite/experimental/shlo/ops/negate_test.cc
index 2cafff63c37..16b05570372 100644
--- a/tensorflow/lite/experimental/shlo/ops/negate_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/negate_test.cc
@@ -70,11 +70,11 @@ TYPED_TEST(NegateTest, ArithmeticTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), negate_ref);
@@ -103,13 +103,13 @@ TYPED_TEST(QuantizedNegateTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/not_test.cc b/tensorflow/lite/experimental/shlo/ops/not_test.cc
index 6a036867895..457fa3d093e 100644
--- a/tensorflow/lite/experimental/shlo/ops/not_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/not_test.cc
@@ -84,11 +84,11 @@ TYPED_TEST(BoolAndIntNotTest, BoolAndIntTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), not_ref);
diff --git a/tensorflow/lite/experimental/shlo/ops/or_test.cc b/tensorflow/lite/experimental/shlo/ops/or_test.cc
index b0fcba048d1..d44e857f5b1 100644
--- a/tensorflow/lite/experimental/shlo/ops/or_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/or_test.cc
@@ -84,14 +84,14 @@ TYPED_TEST(OrTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(),
diff --git a/tensorflow/lite/experimental/shlo/ops/popcnt_test.cc b/tensorflow/lite/experimental/shlo/ops/popcnt_test.cc
index 7b3664b6564..b420f0e9b20 100644
--- a/tensorflow/lite/experimental/shlo/ops/popcnt_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/popcnt_test.cc
@@ -108,11 +108,11 @@ TYPED_TEST(PopcntTest, IntTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), popcnt_ref);
diff --git a/tensorflow/lite/experimental/shlo/ops/sign_test.cc b/tensorflow/lite/experimental/shlo/ops/sign_test.cc
index 4cec0ccac0e..773a20265a1 100644
--- a/tensorflow/lite/experimental/shlo/ops/sign_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/sign_test.cc
@@ -88,11 +88,11 @@ TYPED_TEST(SignTest, ArithmeticTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), sign_ref);
@@ -121,13 +121,13 @@ TYPED_TEST(QuantizedSignTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/sine_test.cc b/tensorflow/lite/experimental/shlo/ops/sine_test.cc
index 3b8a31ebe3b..20aed2612c9 100644
--- a/tensorflow/lite/experimental/shlo/ops/sine_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/sine_test.cc
@@ -85,9 +85,9 @@ TYPED_TEST(FloatSineTest, FloatTensorsWork) {
   Vector<StorageT> input_data = RandomBuffer<TypeParam::kStorage>(shape);
   Vector<StorageT> output_data(shape.NumElements());
   const TensorType tensor_type =
-      TensorType{.shape = shape, .element_type = TypeParam::kStorage};
-  Tensor input_tensor{.type = tensor_type, .data = input_data.data()};
-  Tensor output_tensor{.type = tensor_type, .data = output_data.data()};
+      TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage};
+  Tensor input_tensor{/*type=*/tensor_type, /*data=*/input_data.data()};
+  Tensor output_tensor{/*type=*/tensor_type, /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), sine_ref);
@@ -113,11 +113,11 @@ TYPED_TEST(QuantizedSineTest, PerTensorWorks) {
   Vector<StorageT> input_data = RandomBuffer<TypeParam::kStorage>(shape);
   Vector<StorageT> output_data(shape.NumElements());
   const QuantizedPerTensorTensorType tensor_type = {
-      .shape = shape,
-      .element_type = QuantizedElementTypePerTensor(
+      /*shape=*/shape,
+      /*element_type=*/QuantizedElementTypePerTensor(
           TypeParam::kStorage, zero_point, TypeParam::kExpressed, scale)};
-  Tensor input_tensor{.type = tensor_type, .data = input_data.data()};
-  Tensor output_tensor{.type = tensor_type, .data = output_data.data()};
+  Tensor input_tensor{/*type=*/tensor_type, /*data=*/input_data.data()};
+  Tensor output_tensor{/*type=*/tensor_type, /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/sqrt_test.cc b/tensorflow/lite/experimental/shlo/ops/sqrt_test.cc
index 6669a643ad4..b7b7063d4be 100644
--- a/tensorflow/lite/experimental/shlo/ops/sqrt_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/sqrt_test.cc
@@ -87,11 +87,11 @@ TYPED_TEST(FloatSqrtTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), sqrt_ref);
@@ -121,13 +121,13 @@ TYPED_TEST(QuantizedSqrtTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/subtract_test.cc b/tensorflow/lite/experimental/shlo/ops/subtract_test.cc
index 66d5372c2fa..e75a83e135d 100644
--- a/tensorflow/lite/experimental/shlo/ops/subtract_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/subtract_test.cc
@@ -79,14 +79,14 @@ TYPED_TEST(SubtractTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/-5, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(), Subtract());
@@ -118,17 +118,17 @@ TYPED_TEST(QuantizedSubtractTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor lhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = lhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = rhs_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/tanh_test.cc b/tensorflow/lite/experimental/shlo/ops/tanh_test.cc
index 465580b2f5d..34787c685a3 100644
--- a/tensorflow/lite/experimental/shlo/ops/tanh_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/tanh_test.cc
@@ -86,11 +86,11 @@ TYPED_TEST(FloatTanhTest, FloatTensorsWork) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), tanh_ref);
@@ -119,13 +119,13 @@ TYPED_TEST(QuantizedTanhTest, PerTensorWorks) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/test_util.h b/tensorflow/lite/experimental/shlo/ops/test_util.h
index bdec109db21..893df67c903 100644
--- a/tensorflow/lite/experimental/shlo/ops/test_util.h
+++ b/tensorflow/lite/experimental/shlo/ops/test_util.h
@@ -398,7 +398,7 @@ struct SupportedOpAttributes {
 // a tensor.
 template <DataType storage_type>
 TensorTypeVariant TensorTypeFor(TestParam<storage_type>, const Shape& shape) {
-  return TensorType{.shape = shape, .element_type = storage_type};
+  return TensorType{/*shape=*/shape, /*element_type=*/storage_type};
 }
 
 // Builds a per tensor QuantizedTensorType object and returns it in a variant
@@ -418,8 +418,8 @@ TensorTypeVariant TensorTypeFor(
   StorageType<storage_type> zero_point =
       StorageType<storage_type>(storage_dist(rd));
   return QuantizedPerTensorTensorType{
-      .shape = shape,
-      .element_type = QuantizedElementTypePerTensor(storage_type, zero_point,
+      /*shape=*/shape,
+      /*element_type=*/QuantizedElementTypePerTensor(storage_type, zero_point,
                                                     expressed_type, scale)};
 }
 
@@ -432,8 +432,8 @@ TensorTypeVariant TensorTypeFor(
     PerAxis<TestParam<storage_type, expressed_type>, axis>,
     const Shape& shape) {
   return QuantizedPerAxisTensorType{
-      .shape = shape,
-      .element_type = QuantizedElementTypePerAxis(storage_type, {},
+      /*shape=*/shape,
+      /*element_type=*/QuantizedElementTypePerAxis(storage_type, {},
                                                   expressed_type, {}, axis)};
 }
 
diff --git a/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test.cc b/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test.cc
index fd4d3125ebe..4e73600e4a5 100644
--- a/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test.cc
@@ -61,11 +61,11 @@ TYPED_TEST(UnaryElementWiseTest, NonQuantizedWithAbs) {
   Vector<StorageT> output_data(shape.NumElements());
 
   Tensor input_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = input_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(input_data, expected_data.begin(), Abs());
@@ -94,13 +94,13 @@ TYPED_TEST(QuantizedUnaryElementWiseTest, QuantizedPerTensorWithAbs) {
       QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,
                                     TypeParam::kExpressed, scale);
   Tensor input_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerTensorTensorType{.shape = shape,
-                                           .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerTensorTensorType{/*shape=*/shape,
+                                           /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
@@ -144,13 +144,13 @@ TYPED_TEST(QuantizedUnaryElementWiseTest, QuantizedPerAxisWithAbs) {
       TypeParam::kStorage, zero_points_data, TypeParam::kExpressed, scales_data,
       quantized_dimension);
   Tensor input_tensor{
-      .type = QuantizedPerAxisTensorType{.shape = shape,
-                                         .element_type = tensor_type},
-      .data = input_data.data()};
+      /*type=*/QuantizedPerAxisTensorType{/*shape=*/shape,
+                                         /*element_type=*/tensor_type},
+      /*data=*/input_data.data()};
   Tensor output_tensor{
-      .type = QuantizedPerAxisTensorType{.shape = shape,
-                                         .element_type = tensor_type},
-      .data = output_data.data()};
+      /*type=*/QuantizedPerAxisTensorType{/*shape=*/shape,
+                                         /*element_type=*/tensor_type},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(
diff --git a/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test_util.h b/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test_util.h
index 2f5fab7a712..1370775df57 100644
--- a/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test_util.h
+++ b/tensorflow/lite/experimental/shlo/ops/unary_elementwise_test_util.h
@@ -148,13 +148,13 @@ class UnaryElementwiseOpShapePropagationTest : public ::testing::Test {
 
   Op op_ = Create(typename Op::Attributes{});
   Tensor input_tensor_ = {
-      .type = TensorType{.shape = Shape({2, 3, 4}),
-                         .element_type = SupportedOpDataType<Op>::kStorageType},
-      .data = nullptr};
+      /*type=*/TensorType{/*shape=*/Shape({2, 3, 4}),
+                         /*element_type=*/SupportedOpDataType<Op>::kStorageType},
+      /*data=*/nullptr};
   Tensor output_tensor_ = {
-      .type = TensorType{.shape = Shape(),
-                         .element_type = SupportedOpDataType<Op>::kStorageType},
-      .data = nullptr};
+      /*type=*/TensorType{/*shape=*/Shape(),
+                         /*element_type=*/SupportedOpDataType<Op>::kStorageType},
+      /*data=*/nullptr};
 };
 
 TYPED_TEST_SUITE_P(UnaryElementwiseOpShapePropagationTest);
@@ -216,10 +216,10 @@ TYPED_TEST_P(UnaryElementwiseSameBaselineElementTypeConstraintTest,
   using OperandTypeDesc = std::tuple_element_t<1, TypeParam>;
   using ResultTypeDesc = std::tuple_element_t<2, TypeParam>;
   const Shape shape({2, 3, 4});
-  Tensor input_tensor{.type = TensorTypeFor(OperandTypeDesc{}, shape),
-                      .data = nullptr};
-  Tensor output_tensor{.type = TensorTypeFor(ResultTypeDesc{}, shape),
-                       .data = nullptr};
+  Tensor input_tensor{/*type=*/TensorTypeFor(OperandTypeDesc{}, shape),
+                      /*data=*/nullptr};
+  Tensor output_tensor{/*type=*/TensorTypeFor(ResultTypeDesc{}, shape),
+                       /*data=*/nullptr};
   auto op = Create(typename Op::Attributes{});
   const absl::Status status = Prepare(op, input_tensor, output_tensor);
   EXPECT_THAT(status, shlo_ref::testing::StatusIs(
@@ -243,8 +243,8 @@ TYPED_TEST_SUITE_P(UnaryElementwiseUnsupportedTypeTest);
 TYPED_TEST_P(UnaryElementwiseUnsupportedTypeTest, PrepareRaisesAnError) {
   using Op = std::tuple_element_t<0, TypeParam>;
   using TypeDesc = std::tuple_element_t<1, TypeParam>;
-  Tensor input_tensor{.type = TensorTypeFor(TypeDesc{}, Shape({2, 3, 4})),
-                      .data = nullptr};
+  Tensor input_tensor{/*type=*/TensorTypeFor(TypeDesc{}, Shape({2, 3, 4})),
+                      /*data=*/nullptr};
   Tensor output_tensor = input_tensor;
   auto op = Create(typename Op::Attributes{});
   const absl::Status status = Prepare(op, input_tensor, output_tensor);
diff --git a/tensorflow/lite/experimental/shlo/ops/xor_test.cc b/tensorflow/lite/experimental/shlo/ops/xor_test.cc
index 591b185327a..e4ba439eb32 100644
--- a/tensorflow/lite/experimental/shlo/ops/xor_test.cc
+++ b/tensorflow/lite/experimental/shlo/ops/xor_test.cc
@@ -89,14 +89,14 @@ TYPED_TEST(XorTest, ArithmeticTestTypesTensorsWork) {
       RandomBuffer<TypeParam::kStorage>(shape, /*min=*/1, /*max=*/5);
   Vector<StorageT> output_data(shape.NumElements());
   Tensor lhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = lhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/lhs_data.data()};
   Tensor rhs_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = rhs_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/rhs_data.data()};
   Tensor output_tensor{
-      .type = TensorType{.shape = shape, .element_type = TypeParam::kStorage},
-      .data = output_data.data()};
+      /*type=*/TensorType{/*shape=*/shape, /*element_type=*/TypeParam::kStorage},
+      /*data=*/output_data.data()};
 
   Vector<StorageT> expected_data(shape.NumElements());
   absl::c_transform(lhs_data, rhs_data, expected_data.begin(),
diff --git a/tensorflow/lite/experimental/shlo/quantized_tensor_element_type_test.cc b/tensorflow/lite/experimental/shlo/quantized_tensor_element_type_test.cc
index 10c5a878ddd..dd3df26c302 100644
--- a/tensorflow/lite/experimental/shlo/quantized_tensor_element_type_test.cc
+++ b/tensorflow/lite/experimental/shlo/quantized_tensor_element_type_test.cc
@@ -107,20 +107,20 @@ struct QuantizationPair {
 };
 
 std::vector<QuantizationPair> ValidQuantizationTypePairs() {
-  return {QuantizationPair{.storage_type = DataType::kSI4,
-                           .expressed_type = DataType::kBF16},
-          QuantizationPair{.storage_type = DataType::kSI4,
-                           .expressed_type = DataType::kF16},
-          QuantizationPair{.storage_type = DataType::kSI4,
-                           .expressed_type = DataType::kF32},
-          QuantizationPair{.storage_type = DataType::kSI8,
-                           .expressed_type = DataType::kBF16},
-          QuantizationPair{.storage_type = DataType::kSI8,
-                           .expressed_type = DataType::kF16},
-          QuantizationPair{.storage_type = DataType::kSI8,
-                           .expressed_type = DataType::kF32},
-          QuantizationPair{.storage_type = DataType::kSI16,
-                           .expressed_type = DataType::kF32}};
+  return {QuantizationPair{/*storage_type=*/DataType::kSI4,
+                           /*expressed_type=*/DataType::kBF16},
+          QuantizationPair{/*storage_type=*/DataType::kSI4,
+                           /*expressed_type=*/DataType::kF16},
+          QuantizationPair{/*storage_type=*/DataType::kSI4,
+                           /*expressed_type=*/DataType::kF32},
+          QuantizationPair{/*storage_type=*/DataType::kSI8,
+                           /*expressed_type=*/DataType::kBF16},
+          QuantizationPair{/*storage_type=*/DataType::kSI8,
+                           /*expressed_type=*/DataType::kF16},
+          QuantizationPair{/*storage_type=*/DataType::kSI8,
+                           /*expressed_type=*/DataType::kF32},
+          QuantizationPair{/*storage_type=*/DataType::kSI16,
+                           /*expressed_type=*/DataType::kF32}};
 }
 
 struct PerTensorTest : testing::TestWithParam<QuantizationPair> {
diff --git a/tensorflow/lite/experimental/shlo/tensor_with_data.h b/tensorflow/lite/experimental/shlo/tensor_with_data.h
index e9508573b9b..acd5f06fb13 100644
--- a/tensorflow/lite/experimental/shlo/tensor_with_data.h
+++ b/tensorflow/lite/experimental/shlo/tensor_with_data.h
@@ -38,7 +38,7 @@ class TensorWithData {
   static TensorWithData Create(
       Shape shape, absl::Span<const StorageType<storage_type>> data) {
     Tensor tensor{
-        TensorType{.shape = std::move(shape), .element_type = storage_type}};
+        TensorType{/*shape=*/std::move(shape), /*element_type=*/storage_type}};
     std::vector<std::byte> buffer(tensor.SizeInBytes());
     std::memcpy(buffer.data(), data.data(), tensor.SizeInBytes());
     return TensorWithData(std::move(tensor), std::move(buffer));
@@ -55,8 +55,8 @@ class TensorWithData {
     using ExpressedT = typename Storage<expressed_type>::Type;
 
     Tensor tensor{QuantizedPerTensorTensorType{
-        .shape = std::move(shape),
-        .element_type = QuantizedElementTypePerTensor(storage_type, zero_point,
+        /*shape=*/std::move(shape),
+        /*element_type=*/QuantizedElementTypePerTensor(storage_type, zero_point,
                                                       expressed_type, scale)}};
 
     const ExpressedT scale_inv = ExpressedT(1.0) / scale;
diff --git a/tensorflow/lite/java/src/test/native/interpreter_test_jni.cc b/tensorflow/lite/java/src/test/native/interpreter_test_jni.cc
index 17c32e6a6fe..43bd1087479 100644
--- a/tensorflow/lite/java/src/test/native/interpreter_test_jni.cc
+++ b/tensorflow/lite/java/src/test/native/interpreter_test_jni.cc
@@ -27,9 +27,9 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForDelegate(
     JNIEnv* env, jclass clazz) {
   // A simple op which outputs a tensor with values of 7.
   static TfLiteRegistration registration = {
-      .init = nullptr,
-      .free = nullptr,
-      .prepare =
+      /*init=*/nullptr,
+      /*free=*/nullptr,
+      /*prepare=*/
           [](TfLiteContext* context, TfLiteNode* node) {
             const TfLiteTensor* input;
             TF_LITE_ENSURE_OK(context,
@@ -41,7 +41,7 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForDelegate(
             output->type = kTfLiteFloat32;
             return context->ResizeTensor(context, output, output_dims);
           },
-      .invoke =
+      /*invoke=*/
           [](TfLiteContext* context, TfLiteNode* node) {
             TfLiteTensor* output;
             TF_LITE_ENSURE_OK(context,
@@ -50,14 +50,14 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForDelegate(
                       output->data.f + tflite::NumElements(output), 7.0f);
             return kTfLiteOk;
           },
-      .profiling_string = nullptr,
-      .builtin_code = 0,
-      .custom_name = "",
-      .version = 1,
+      /*profiling_string=*/nullptr,
+      /*builtin_code=*/0,
+      /*custom_name=*/"",
+      /*version=*/1,
   };
   static TfLiteDelegate delegate = {
-      .data_ = nullptr,
-      .Prepare = [](TfLiteContext* context,
+      /*data_=*/nullptr,
+      /*Prepare=*/[](TfLiteContext* context,
                     TfLiteDelegate* delegate) -> TfLiteStatus {
         TfLiteIntArray* execution_plan;
         TF_LITE_ENSURE_STATUS(
@@ -71,10 +71,10 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForDelegate(
         }
         return kTfLiteOk;
       },
-      .CopyFromBufferHandle = nullptr,
-      .CopyToBufferHandle = nullptr,
-      .FreeBufferHandle = nullptr,
-      .flags = kTfLiteDelegateFlagsAllowDynamicTensors,
+      /*CopyFromBufferHandle=*/nullptr,
+      /*CopyToBufferHandle=*/nullptr,
+      /*FreeBufferHandle=*/nullptr,
+      /*flags=*/kTfLiteDelegateFlagsAllowDynamicTensors,
   };
   return reinterpret_cast<jlong>(&delegate);
 }
@@ -84,13 +84,13 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForInvalidDelegate(
     JNIEnv* env, jclass clazz) {
   // A simple delegate that fails during preparation.
   static TfLiteDelegate delegate = {
-      .data_ = nullptr,
-      .Prepare = [](TfLiteContext* context, TfLiteDelegate* delegate)
+      /*data_=*/nullptr,
+      /*Prepare=*/[](TfLiteContext* context, TfLiteDelegate* delegate)
           -> TfLiteStatus { return kTfLiteError; },
-      .CopyFromBufferHandle = nullptr,
-      .CopyToBufferHandle = nullptr,
-      .FreeBufferHandle = nullptr,
-      .flags = kTfLiteDelegateFlagsNone,
+      /*CopyFromBufferHandle=*/nullptr,
+      /*CopyToBufferHandle=*/nullptr,
+      /*FreeBufferHandle=*/nullptr,
+      /*flags=*/kTfLiteDelegateFlagsNone,
   };
   return reinterpret_cast<jlong>(&delegate);
 }
diff --git a/tensorflow/lite/mutable_op_resolver_test.cc b/tensorflow/lite/mutable_op_resolver_test.cc
index 8622579a3c8..0e50e6491c0 100644
--- a/tensorflow/lite/mutable_op_resolver_test.cc
+++ b/tensorflow/lite/mutable_op_resolver_test.cc
@@ -31,11 +31,11 @@ TfLiteStatus DummyInvoke(TfLiteContext* context, TfLiteNode* node) {
 }
 
 TfLiteRegistration* GetDummyRegistration() {
-  static TfLiteRegistration registration = {
-      .init = nullptr,
-      .free = nullptr,
-      .prepare = nullptr,
-      .invoke = DummyInvoke,
+  static TfLiteRegistration registration{
+    /*init=*/nullptr,
+    /*free=*/nullptr,
+    /*prepare=*/nullptr,
+    /*invoke=*/DummyInvoke,
   };
   return &registration;
 }
@@ -55,11 +55,11 @@ void* Dummy2Init(TfLiteContext* context, const char* buffer, size_t length) {
 void Dummy2free(TfLiteContext* context, void* buffer) {}
 
 TfLiteRegistration* GetDummy2Registration() {
-  static TfLiteRegistration registration = {
-      .init = Dummy2Init,
-      .free = Dummy2free,
-      .prepare = Dummy2Prepare,
-      .invoke = Dummy2Invoke,
+  static TfLiteRegistration registration{
+    /*.init=*/ Dummy2Init,
+    /*.free=*/ Dummy2free,
+    /*.prepare=*/ Dummy2Prepare,
+    /*.invoke=*/ Dummy2Invoke,
   };
   return &registration;
 }
diff --git a/tensorflow/lite/toco/tflite/operator_test.cc b/tensorflow/lite/toco/tflite/operator_test.cc
index 8f1d42ad8fb..14fb374fe2d 100644
--- a/tensorflow/lite/toco/tflite/operator_test.cc
+++ b/tensorflow/lite/toco/tflite/operator_test.cc
@@ -569,27 +569,27 @@ TEST_F(OperatorTest, VersioningSpareToDense) {
   Model int32_model;
   Array& int32_array = int32_model.GetOrCreateArray(op.inputs[2]);
   int32_array.data_type = ArrayDataType::kInt32;
-  OperatorSignature int32_signature = {.op = &op, .model = &int32_model};
+  OperatorSignature int32_signature = {/*op=*/&op, /*model=*/&int32_model};
   EXPECT_EQ(base_op->GetVersion(int32_signature), 1);
 
   // Expect version 2 for int64 input.
   Model int64_model;
   Array& int64_array = int64_model.GetOrCreateArray(op.inputs[2]);
   int64_array.data_type = ArrayDataType::kInt64;
-  OperatorSignature int64_signature = {.op = &op, .model = &int64_model};
+  OperatorSignature int64_signature = {/*op=*/&op, /*model=*/&int64_model};
   EXPECT_EQ(base_op->GetVersion(int64_signature), 2);
 
   // Expect version 3 for int8 and uint8 input.
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[2]);
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 3);
 
   Model uint8_model;
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[2]);
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 3);
 }
 
@@ -807,13 +807,13 @@ void SimpleVersioningTest() {
   Model uint8_model;
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[0]);
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[0]);
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 }
 
@@ -829,13 +829,13 @@ void SimpleOutputVersioningTest() {
   Model uint8_model;
   Array& uint8_array = uint8_model.GetOrCreateArray(op.outputs[0]);
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.outputs[0]);
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 }
 
@@ -873,21 +873,21 @@ TEST_F(OperatorTest, VersioningSpaceToBatchNDTest) {
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[0]);
   uint8_array.copy_shape({1, 2, 2, 2});
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[0]);
   int8_array.copy_shape({1, 2, 2, 2});
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 
   Model float_model;
   Array& float_array = float_model.GetOrCreateArray(op.inputs[0]);
   float_array.copy_shape({1, 2, 2});
   float_array.data_type = ArrayDataType::kFloat;
-  OperatorSignature float_signature = {.op = &op, .model = &float_model};
+  OperatorSignature float_signature = {/*op=*/&op, /*model=*/&float_model};
   EXPECT_EQ(base_op->GetVersion(float_signature), 3);
 }
 
@@ -908,19 +908,19 @@ TEST_F(OperatorTest, VersioningUnpackTest) {
   Model int32_model;
   Array& int32_array = int32_model.GetOrCreateArray(op.inputs[0]);
   int32_array.data_type = ArrayDataType::kInt32;
-  OperatorSignature int32_signature = {.op = &op, .model = &int32_model};
+  OperatorSignature int32_signature = {/*op=*/&op, /*model=*/&int32_model};
   EXPECT_EQ(base_op->GetVersion(int32_signature), 1);
 
   Model uint8_model;
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[0]);
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 2);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[0]);
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 }
 
@@ -934,21 +934,21 @@ TEST_F(OperatorTest, VersioningBatchToSpaceNDTest) {
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[0]);
   uint8_array.data_type = ArrayDataType::kUint8;
   uint8_array.copy_shape({1, 2, 2, 2});
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[0]);
   int8_array.data_type = ArrayDataType::kInt8;
   int8_array.copy_shape({1, 2, 2, 2});
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 
   Model float_model;
   Array& float_array = float_model.GetOrCreateArray(op.inputs[0]);
   float_array.copy_shape({1, 2, 2});
   float_array.data_type = ArrayDataType::kFloat;
-  OperatorSignature float_signature = {.op = &op, .model = &float_model};
+  OperatorSignature float_signature = {/*op=*/&op, /*model=*/&float_model};
   EXPECT_EQ(base_op->GetVersion(float_signature), 3);
 }
 
@@ -967,19 +967,19 @@ TEST_F(OperatorTest, VersioningStridedSliceTest) {
   Model uint8_model;
   Array& uint8_array = uint8_model.GetOrCreateArray(op.inputs[0]);
   uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&op, /*model=*/&uint8_model};
   EXPECT_EQ(base_op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
   Array& int8_array = int8_model.GetOrCreateArray(op.inputs[0]);
   int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&op, /*model=*/&int8_model};
   EXPECT_EQ(base_op->GetVersion(int8_signature), 2);
 
   Model bool_model;
   Array& bool_array = bool_model.GetOrCreateArray(op.inputs[0]);
   bool_array.data_type = ArrayDataType::kBool;
-  OperatorSignature bool_signature = {.op = &op, .model = &bool_model};
+  OperatorSignature bool_signature = {/*op=*/&op, /*model=*/&bool_model};
   EXPECT_EQ(base_op->GetVersion(bool_signature), 3);
 
   op.start_indices = {0, 0, 0, 0, 0};
@@ -1006,7 +1006,7 @@ TEST_F(OperatorTest, VersioningSliceTest) {
   Model string_model;
   Array& string_array = string_model.GetOrCreateArray(op.inputs[0]);
   string_array.data_type = ArrayDataType::kString;
-  OperatorSignature string_signature = {.op = &op, .model = &string_model};
+  OperatorSignature string_signature = {/*op=*/&op, /*model=*/&string_model};
   EXPECT_EQ(base_op->GetVersion(string_signature), 3);
 }
 
@@ -1056,7 +1056,7 @@ void SimpleMulVersioningTest(ArrayDataType data_type, float multiplier,
   output.data_type = data_type;
   output.GetOrCreateQuantizationParams().scale = 1.0f / multiplier;
 
-  OperatorSignature signature = {.op = &op, .model = &model};
+  OperatorSignature signature = {/*op=*/&op, /*model=*/&model};
   EXPECT_EQ(base_op->GetVersion(signature), version);
 }
 
@@ -1086,7 +1086,7 @@ void SimpleTwoInputsVersioningTest(ArrayDataType data_type, Shape shape1,
   input1.copy_shape(shape2);
   output.data_type = data_type;
 
-  OperatorSignature signature = {.op = &op, .model = &model};
+  OperatorSignature signature = {/*op=*/&op, /*model=*/&model};
   EXPECT_EQ(base_op->GetVersion(signature), version);
 }
 
@@ -1113,7 +1113,7 @@ void SimpleThreeInputsVersioningTest(ArrayDataType data_type, Shape shape1,
   input2.copy_shape(shape3);
   output.data_type = data_type;
 
-  OperatorSignature signature = {.op = &op, .model = &model};
+  OperatorSignature signature = {/*op=*/&op, /*model=*/&model};
   EXPECT_EQ(base_op->GetVersion(signature), version);
 }
 
@@ -1189,8 +1189,8 @@ TEST_F(OperatorTest, VersioningFullyConnectedTest) {
   Array& output_uint8_array =
       uint8_model.GetOrCreateArray(fully_connected_op.outputs[0]);
   output_uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &fully_connected_op,
-                                       .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&fully_connected_op,
+                                       /*model=*/&uint8_model};
   EXPECT_EQ(op->GetVersion(uint8_signature), 6);
 
   Model int8_model;
@@ -1203,8 +1203,8 @@ TEST_F(OperatorTest, VersioningFullyConnectedTest) {
   Array& output_int8_array =
       int8_model.GetOrCreateArray(fully_connected_op.outputs[0]);
   output_int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &fully_connected_op,
-                                      .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&fully_connected_op,
+                                      /*model=*/&int8_model};
   EXPECT_EQ(op->GetVersion(int8_signature), 6);
 }
 
@@ -1218,29 +1218,29 @@ TEST_F(OperatorTest, VersioningDequantizeTest) {
   Model int16_model;
   Array& input_int16_array = int16_model.GetOrCreateArray(dequant_op.inputs[0]);
   input_int16_array.data_type = ArrayDataType::kInt16;
-  OperatorSignature int16_signature = {.op = &dequant_op,
-                                       .model = &int16_model};
+  OperatorSignature int16_signature = {/*op=*/&dequant_op,
+                                       /*model=*/&int16_model};
   EXPECT_EQ(op->GetVersion(int16_signature), 3);
 
   Model float16_model;
   Array& input_float16_array =
       float16_model.GetOrCreateArray(dequant_op.inputs[0]);
   input_float16_array.data_type = ArrayDataType::kFloat16;
-  OperatorSignature float16_signature = {.op = &dequant_op,
-                                         .model = &float16_model};
+  OperatorSignature float16_signature = {/*op=*/&dequant_op,
+                                         /*model=*/&float16_model};
   EXPECT_EQ(op->GetVersion(float16_signature), 3);
 
   Model int8_model;
   Array& input_int8_array = int8_model.GetOrCreateArray(dequant_op.inputs[0]);
   input_int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &dequant_op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&dequant_op, /*model=*/&int8_model};
   EXPECT_EQ(op->GetVersion(int8_signature), 2);
 
   Model float_model;
   Array& input_float_array = float_model.GetOrCreateArray(dequant_op.inputs[0]);
   input_float_array.data_type = ArrayDataType::kFloat;
-  OperatorSignature float_signature = {.op = &dequant_op,
-                                       .model = &float_model};
+  OperatorSignature float_signature = {/*op=*/&dequant_op,
+                                       /*model=*/&float_model};
   EXPECT_EQ(op->GetVersion(float_signature), 1);
 }
 
@@ -1258,7 +1258,7 @@ TEST_F(OperatorTest, VersioningConv2DTest) {
   filter_uint8_array.data_type = ArrayDataType::kUint8;
   Array& output_uint8_array = uint8_model.GetOrCreateArray(conv_op.outputs[0]);
   output_uint8_array.data_type = ArrayDataType::kUint8;
-  OperatorSignature uint8_signature = {.op = &conv_op, .model = &uint8_model};
+  OperatorSignature uint8_signature = {/*op=*/&conv_op, /*model=*/&uint8_model};
   EXPECT_EQ(op->GetVersion(uint8_signature), 1);
 
   Model int8_model;
@@ -1268,7 +1268,7 @@ TEST_F(OperatorTest, VersioningConv2DTest) {
   filter_int8_array.data_type = ArrayDataType::kInt8;
   Array& output_int8_array = int8_model.GetOrCreateArray(conv_op.outputs[0]);
   output_int8_array.data_type = ArrayDataType::kInt8;
-  OperatorSignature int8_signature = {.op = &conv_op, .model = &int8_model};
+  OperatorSignature int8_signature = {/*op=*/&conv_op, /*model=*/&int8_model};
   EXPECT_EQ(op->GetVersion(int8_signature), 3);
 
   Model float_model;
@@ -1278,7 +1278,7 @@ TEST_F(OperatorTest, VersioningConv2DTest) {
   filter_int8_array1.data_type = ArrayDataType::kInt8;
   Array& output_float_array = float_model.GetOrCreateArray(conv_op.outputs[0]);
   output_float_array.data_type = ArrayDataType::kFloat;
-  OperatorSignature float_signature = {.op = &conv_op, .model = &float_model};
+  OperatorSignature float_signature = {/*op=*/&conv_op, /*model=*/&float_model};
   EXPECT_EQ(op->GetVersion(float_signature), 2);
 }
 
@@ -1292,16 +1292,16 @@ TEST_F(OperatorTest, VersioningFloorDivOperatorTest) {
   Array& input_int32_array =
       int32_model.GetOrCreateArray(floordiv_op.inputs[0]);
   input_int32_array.data_type = ArrayDataType::kInt32;
-  OperatorSignature int32_signature = {.op = &floordiv_op,
-                                       .model = &int32_model};
+  OperatorSignature int32_signature = {/*op=*/&floordiv_op,
+                                       /*model=*/&int32_model};
   EXPECT_EQ(op->GetVersion(int32_signature), 1);
 
   Model float_model;
   Array& input_float_array =
       float_model.GetOrCreateArray(floordiv_op.inputs[0]);
   input_float_array.data_type = ArrayDataType::kFloat;
-  OperatorSignature float_signature = {.op = &floordiv_op,
-                                       .model = &float_model};
+  OperatorSignature float_signature = {/*op=*/&floordiv_op,
+                                       /*model=*/&float_model};
   EXPECT_EQ(op->GetVersion(float_signature), 2);
 }
 
diff --git a/tensorflow/lite/tools/delegates/coreml_delegate_provider.cc b/tensorflow/lite/tools/delegates/coreml_delegate_provider.cc
index 0017cfeca9d..b438a2762f4 100644
--- a/tensorflow/lite/tools/delegates/coreml_delegate_provider.cc
+++ b/tensorflow/lite/tools/delegates/coreml_delegate_provider.cc
@@ -81,7 +81,7 @@ TfLiteDelegatePtr CoreMlDelegateProvider::CreateTfLiteDelegate(
 #if defined(REAL_IPHONE_DEVICE)
   if (params.Get<bool>("use_coreml")) {
     TfLiteCoreMlDelegateOptions coreml_opts = {
-        .enabled_devices = TfLiteCoreMlDelegateAllDevices};
+        /*enabled_devices=*/TfLiteCoreMlDelegateAllDevices};
     coreml_opts.coreml_version = params.Get<int>("coreml_version");
     coreml_opts.max_delegated_partitions =
         params.Get<int>("max_delegated_partitions");
diff --git a/tensorflow/lite/tools/evaluation/utils.cc b/tensorflow/lite/tools/evaluation/utils.cc
index 6628911f794..fbe7bad7961 100644
--- a/tensorflow/lite/tools/evaluation/utils.cc
+++ b/tensorflow/lite/tools/evaluation/utils.cc
@@ -281,7 +281,7 @@ TfLiteDelegatePtr CreateXNNPACKDelegate(
 TfLiteDelegatePtr CreateCoreMlDelegate() {
 #ifdef REAL_IPHONE_DEVICE
   TfLiteCoreMlDelegateOptions coreml_options = {
-      .enabled_devices = TfLiteCoreMlDelegateAllDevices};
+      /*enabled_devices=*/TfLiteCoreMlDelegateAllDevices};
   TfLiteDelegate* delegate = TfLiteCoreMlDelegateCreate(&coreml_options);
   if (!delegate) {
     return tools::CreateNullDelegate();
diff --git a/tensorflow/lite/tools/optimize/quantize_weights_test.cc b/tensorflow/lite/tools/optimize/quantize_weights_test.cc
index 8f7a1150bef..73f16249cd2 100644
--- a/tensorflow/lite/tools/optimize/quantize_weights_test.cc
+++ b/tensorflow/lite/tools/optimize/quantize_weights_test.cc
@@ -467,8 +467,8 @@ TEST_F(QuantizeWeightsTest, VerifyCustomOpQuantizationDequantize) {
   // be quantized.
   CustomOpMap custom_op_map;
   custom_op_map["CustomTestOp"] = {
-      .quantizable_input_indices = {1},
-      .is_hybrid = false,
+      /*quantizable_input_indices=*/{1},
+      /*is_hybrid=*/false,
   };
 
   flatbuffers::FlatBufferBuilder builder;
@@ -515,8 +515,8 @@ TEST_F(QuantizeWeightsTest, VerifyCustomOpQuantizationHybrid) {
   // be quantized.
   CustomOpMap custom_op_map;
   custom_op_map["CustomTestOp"] = {
-      .quantizable_input_indices = {1},
-      .is_hybrid = true,
+      /*quantizable_input_indices=*/{1},
+      /*is_hybrid=*/true,
   };
 
   flatbuffers::FlatBufferBuilder builder;
diff --git a/tensorflow/lite/tools/versioning/op_version_test.cc b/tensorflow/lite/tools/versioning/op_version_test.cc
index acddf0d0de9..2e89bbd282e 100644
--- a/tensorflow/lite/tools/versioning/op_version_test.cc
+++ b/tensorflow/lite/tools/versioning/op_version_test.cc
@@ -103,29 +103,29 @@ std::vector<OpSignatureTensorSpec> CreateOpSignatureTensorSpecs(
 
 TEST(OpVersionTest, VersioningSpareToDense) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SPARSE_TO_DENSE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SPARSE_TO_DENSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8, kTfLiteInt8}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SPARSE_TO_DENSE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SPARSE_TO_DENSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteUInt8, kTfLiteUInt8, kTfLiteUInt8}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SPARSE_TO_DENSE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SPARSE_TO_DENSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt64, kTfLiteInt64, kTfLiteInt64}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SPARSE_TO_DENSE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SPARSE_TO_DENSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt32, kTfLiteInt32, kTfLiteInt32}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
@@ -135,14 +135,14 @@ TEST(OpVersionTest, VersioningSpareToDense) {
 // version.
 void SimpleVersioningTest(BuiltinOperator op) {
   OpSignature fake_op_sig = {
-      .op = op,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/op,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = op,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*op=*/op,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
@@ -151,8 +151,8 @@ void SimpleVersioningTest(BuiltinOperator op) {
 // op has 3 versions and the input type includes kTfLiteInt16.
 void SimpleVersioningTestExtended(BuiltinOperator op) {
   OpSignature fake_op_sig = {
-      .op = op,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/op,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
@@ -162,16 +162,16 @@ void SimpleVersioningTestExtended(BuiltinOperator op) {
 // Test version for a simple Op with 2 versions and the output type controls the
 void SimpleOutputVersioningTest(BuiltinOperator op) {
   OpSignature fake_op_sig = {
-      .op = op,
-      .inputs = std::vector<OpSignatureTensorSpec>{},
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/op,
+      /*inputs=*/std::vector<OpSignatureTensorSpec>{},
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = op,
-      .inputs = std::vector<OpSignatureTensorSpec>{},
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*op=*/op,
+      /*inputs=*/std::vector<OpSignatureTensorSpec>{},
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
@@ -179,8 +179,8 @@ void SimpleOutputVersioningTest(BuiltinOperator op) {
 TEST(OpVersionTest, VersioningEqualTest) {
   SimpleVersioningTest(BuiltinOperator_EQUAL);
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_EQUAL,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteString),
+      /*op=*/BuiltinOperator_EQUAL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteString),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 }
@@ -188,8 +188,8 @@ TEST(OpVersionTest, VersioningEqualTest) {
 TEST(OpVersionTest, VersioningNotEqualTest) {
   SimpleVersioningTest(BuiltinOperator_NOT_EQUAL);
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_NOT_EQUAL,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteString),
+      /*op=*/BuiltinOperator_NOT_EQUAL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteString),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 }
@@ -212,7 +212,7 @@ TEST(OpVersionTest, VersioningGreaterEqualTest) {
 
 TEST(OpVersionTest, VersioningSpaceToBatchNDTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SPACE_TO_BATCH_ND,
+      /*op=*/BuiltinOperator_SPACE_TO_BATCH_ND,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16, 3);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
@@ -257,20 +257,20 @@ TEST(OpVersionTest, VersioningPackTest) {
 
 TEST(OpVersionTest, VersioningUnpackTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_UNPACK,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_UNPACK,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_UNPACK,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*op=*/BuiltinOperator_UNPACK,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_UNPACK,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32),
+      /*op=*/BuiltinOperator_UNPACK,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
@@ -287,33 +287,33 @@ TEST(OpVersionTest, VersioningRangeTest) {
 
 TEST(OpVersionTest, VersioningReluTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_RELU,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_RELU,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_RELU,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_RELU,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_RELU,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*op=*/BuiltinOperator_RELU,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_RELU,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32),
+      /*op=*/BuiltinOperator_RELU,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
 
 TEST(OpVersionTest, VersioningBatchToSpaceNDTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_BATCH_TO_SPACE_ND,
+      /*op=*/BuiltinOperator_BATCH_TO_SPACE_ND,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16, 3);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
@@ -367,33 +367,33 @@ TEST(OpVersionTest, VersioningSpaceToDepthTest) {
 
 TEST(OpVersionTest, VersioningSliceTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SLICE,
+      /*op=*/BuiltinOperator_SLICE,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16, 5);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SLICE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_SLICE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16, 4);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SLICE,
+      /*op=*/BuiltinOperator_SLICE,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteString, 4);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SLICE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_SLICE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8, 4);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SLICE,
+      /*op=*/BuiltinOperator_SLICE,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8, 4);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
@@ -414,7 +414,7 @@ TEST(OpVersionTest, VersioningL2NormTest) {
 
 TEST(OpVersionTest, VersioningMaxTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_MAXIMUM,
+      /*op=*/BuiltinOperator_MAXIMUM,
   };
 
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8, 4, 5);
@@ -425,7 +425,7 @@ TEST(OpVersionTest, VersioningMaxTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_MAXIMUM,
+      /*op=*/BuiltinOperator_MAXIMUM,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8, 4, 5);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
@@ -435,7 +435,7 @@ TEST(OpVersionTest, VersioningMaxTest) {
 
 TEST(OpVersionTest, VersioningMinTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_MINIMUM,
+      /*op=*/BuiltinOperator_MINIMUM,
   };
 
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8, 4, 5);
@@ -446,7 +446,7 @@ TEST(OpVersionTest, VersioningMinTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_MINIMUM,
+      /*op=*/BuiltinOperator_MINIMUM,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8, 4, 5);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
@@ -490,10 +490,10 @@ TEST(OpVersionTest, VersioningReduceProdTest) {
 TEST(OpVersionTest, VersioningAddTest) {
   TfLiteAddParams add_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_ADD,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&add_params)};
+      /*op=*/BuiltinOperator_ADD,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&add_params)};
   add_params.pot_scale_int16 = false;
   fake_op_sig.ext_options.add.input_quantized = true;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
@@ -507,10 +507,10 @@ TEST(OpVersionTest, VersioningAddTest) {
 TEST(OpVersionTest, VersioningSubTest) {
   TfLiteSubParams sub_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SUB,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&sub_params)};
+      /*op=*/BuiltinOperator_SUB,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&sub_params)};
   sub_params.pot_scale_int16 = false;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
 
@@ -555,8 +555,8 @@ TEST(OpVersionTest, VersioningMUL5Test) {
 
 TEST(OpVersionTest, VersioningSub4Test) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SUB,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt64),
+      /*op=*/BuiltinOperator_SUB,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt64),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 }
@@ -564,10 +564,10 @@ TEST(OpVersionTest, VersioningSub4Test) {
 void SimpleMulVersioningTest(TfLiteType data_type, float multiplier,
                              int version) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_MUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_MUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{data_type, data_type}),
-      .outputs = CreateOpSignatureTensorSpecs(data_type),
+      /*outputs=*/CreateOpSignatureTensorSpecs(data_type),
   };
   fake_op_sig.ext_options.mul = {1.0f, 1.0f, 1.0f / multiplier};
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), version);
@@ -655,33 +655,33 @@ TEST(OpVersionTest, VersioningRelu6Test) {
 TEST(OpVersionTest, VersioningFullyConnectedTest) {
   TfLiteFullyConnectedParams fully_connected_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteUInt8, kTfLiteUInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fully_connected_params.weights_format =
       kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 6);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fully_connected_params.weights_format =
       kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 6);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fully_connected_params.weights_format =
       kTfLiteFullyConnectedWeightsFormatDefault;
@@ -689,11 +689,11 @@ TEST(OpVersionTest, VersioningFullyConnectedTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 8);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fully_connected_params.asymmetric_quantize_inputs = false;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
@@ -701,21 +701,21 @@ TEST(OpVersionTest, VersioningFullyConnectedTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 9);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fully_connected_params.quantized_bias_type = kTfLiteInt32;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 11);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FULLY_CONNECTED,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_FULLY_CONNECTED,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&fully_connected_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&fully_connected_params),
   };
   fake_op_sig.ext_options.fully_connected.is_per_channel_quantized = true;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 12);
@@ -723,20 +723,20 @@ TEST(OpVersionTest, VersioningFullyConnectedTest) {
 
 TEST(OpVersionTest, VersioningDequantizeTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_DEQUANTIZE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_DEQUANTIZE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_DEQUANTIZE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat16),
+      /*op=*/BuiltinOperator_DEQUANTIZE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_DEQUANTIZE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_DEQUANTIZE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
@@ -744,8 +744,8 @@ TEST(OpVersionTest, VersioningDequantizeTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
 
   fake_op_sig = {
-      .op = BuiltinOperator_DEQUANTIZE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*op=*/BuiltinOperator_DEQUANTIZE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
@@ -771,34 +771,34 @@ TEST(OpVersionTest, VersioningQuantizeTest) {
 
 TEST(OpVersionTest, VersioningConv2DTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteUInt8, kTfLiteUInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   fake_op_sig.ext_options.conv_2d.is_per_channel_quantized = true;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
@@ -813,11 +813,11 @@ TEST(OpVersionTest, VersioningConv2DTest) {
 
   TfLiteConvParams conv_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&conv_params),
   };
   conv_params.quantized_bias_type = kTfLiteInt32;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 8);
@@ -825,64 +825,64 @@ TEST(OpVersionTest, VersioningConv2DTest) {
 
 TEST(OpVersionTest, VersioningFloorDivOperatorTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_FLOOR_DIV,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32),
+      /*op=*/BuiltinOperator_FLOOR_DIV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FLOOR_DIV,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*op=*/BuiltinOperator_FLOOR_DIV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FLOOR_DIV,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_FLOOR_DIV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 }
 
 TEST(OpVersionTest, VersioningFloorModOperatorTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_FLOOR_MOD,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32),
+      /*op=*/BuiltinOperator_FLOOR_MOD,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_FLOOR_MOD,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_FLOOR_MOD,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 }
 
 TEST(OpVersionTest, VersioningTransposeConvOperatorTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteUInt8}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt32, kTfLiteInt8, kTfLiteInt8}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
           kTfLiteInt32, kTfLiteInt8, kTfLiteInt8, kTfLiteInt32}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   const auto none_type = kTfLiteNoType;
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
           kTfLiteInt32, kTfLiteInt8, kTfLiteInt8, none_type}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
@@ -890,20 +890,20 @@ TEST(OpVersionTest, VersioningTransposeConvOperatorTest) {
   TfLiteTransposeConvParams transpose_conv_params = {};
   transpose_conv_params.activation = kTfLiteActRelu;
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
           kTfLiteInt32, kTfLiteInt8, kTfLiteInt8, none_type}),
-      .builtin_data = reinterpret_cast<void*>(&transpose_conv_params),
+      /*builtin_data=*/reinterpret_cast<void*>(&transpose_conv_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 
   transpose_conv_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE_CONV,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_TRANSPOSE_CONV,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&transpose_conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&transpose_conv_params),
   };
   transpose_conv_params.quantized_bias_type = kTfLiteInt32;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
@@ -912,22 +912,22 @@ TEST(OpVersionTest, VersioningTransposeConvOperatorTest) {
 TEST(OpVersionTest, VersioningSVDFOperatorTest) {
   TfLiteSVDFParams svdf_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SVDF,
-      .inputs = CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
+      /*op=*/BuiltinOperator_SVDF,
+      /*inputs=*/CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
           kTfLiteFloat32, kTfLiteFloat32, kTfLiteFloat32, kTfLiteFloat32,
           kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&svdf_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&svdf_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_SVDF,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SVDF,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8, kTfLiteFloat32,
                                   kTfLiteFloat32, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&svdf_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&svdf_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
   svdf_params.asymmetric_quantize_inputs = true;
@@ -935,11 +935,11 @@ TEST(OpVersionTest, VersioningSVDFOperatorTest) {
 
   svdf_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_SVDF,
-      .inputs = CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
+      /*op=*/BuiltinOperator_SVDF,
+      /*inputs=*/CreateOpSignatureTensorSpecs(std::vector<TfLiteType>{
           kTfLiteInt8, kTfLiteInt8, kTfLiteInt32, kTfLiteInt32, kTfLiteInt16}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&svdf_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&svdf_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 }
@@ -947,11 +947,11 @@ TEST(OpVersionTest, VersioningSVDFOperatorTest) {
 TEST(OpVersionTest, VersioningDepthwiseConv2DTest) {
   TfLiteDepthwiseConvParams depthwise_conv_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_DEPTHWISE_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_DEPTHWISE_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&depthwise_conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&depthwise_conv_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
   fake_op_sig.ext_options.depthwise_conv_2d.is_per_channel_quantized = true;
@@ -959,31 +959,31 @@ TEST(OpVersionTest, VersioningDepthwiseConv2DTest) {
 
   depthwise_conv_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_DEPTHWISE_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_DEPTHWISE_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&depthwise_conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&depthwise_conv_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_DEPTHWISE_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_DEPTHWISE_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&depthwise_conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&depthwise_conv_params),
   };
   depthwise_conv_params.dilation_width_factor = 2;
   depthwise_conv_params.dilation_height_factor = 2;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_DEPTHWISE_CONV_2D,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_DEPTHWISE_CONV_2D,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&depthwise_conv_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&depthwise_conv_params),
   };
   depthwise_conv_params.dilation_width_factor = 1;
   depthwise_conv_params.dilation_height_factor = 1;
@@ -991,26 +991,26 @@ TEST(OpVersionTest, VersioningDepthwiseConv2DTest) {
 }
 TEST(OpVersionTest, VersioningTileOperatorTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_TILE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt32),
+      /*op=*/BuiltinOperator_TILE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TILE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteString),
+      /*op=*/BuiltinOperator_TILE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteString),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 }
 TEST(OpVersionTest, VersioningTransposeTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_TRANSPOSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE,
+      /*op=*/BuiltinOperator_TRANSPOSE,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteBool, 5);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
@@ -1018,56 +1018,56 @@ TEST(OpVersionTest, VersioningTransposeTest) {
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_TRANSPOSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_TRANSPOSE,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8),
+      /*op=*/BuiltinOperator_TRANSPOSE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteUInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 }
 TEST(OpVersionTest, VersioningGatherNdOperatorTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_GATHER_ND,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_GATHER_ND,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt32, kTfLiteInt32}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   fake_op_sig = {
-      .op = BuiltinOperator_GATHER_ND,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_GATHER_ND,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteString, kTfLiteInt32}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   fake_op_sig = {
-      .op = BuiltinOperator_GATHER_ND,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_GATHER_ND,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt32}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_GATHER_ND,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_GATHER_ND,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt32, kTfLiteInt16}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 
   fake_op_sig = {
-      .op = BuiltinOperator_GATHER_ND,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_GATHER_ND,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteBool, kTfLiteInt16}),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);
 }
 TEST(OpVersionTest, VersioningDivTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_DIV,
+      /*op=*/BuiltinOperator_DIV,
   };
   fake_op_sig.inputs = CreateOpSignatureTensorSpecs(kTfLiteUInt8, 5, 4);
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
@@ -1104,11 +1104,11 @@ TEST(OpVersionTest, VersioningResizeBilinearTest) {
   // Default.
   TfLiteResizeBilinearParams resize_bilinear_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_BILINEAR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_BILINEAR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&resize_bilinear_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_bilinear_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
@@ -1124,11 +1124,11 @@ TEST(OpVersionTest, VersioningResizeBilinearTest) {
   // int8 input is version 2.
   resize_bilinear_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_BILINEAR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_BILINEAR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&resize_bilinear_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_bilinear_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
@@ -1138,11 +1138,11 @@ TEST(OpVersionTest, VersioningResizeBilinearTest) {
   // int16 input is version 4.
   resize_bilinear_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_BILINEAR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_BILINEAR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&resize_bilinear_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_bilinear_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 }
@@ -1150,11 +1150,11 @@ TEST(OpVersionTest, VersioningResizeNearestNeighborTest) {
   // Default.
   TfLiteResizeNearestNeighborParams resize_nearest_neighbor_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&resize_nearest_neighbor_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_nearest_neighbor_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
@@ -1170,11 +1170,11 @@ TEST(OpVersionTest, VersioningResizeNearestNeighborTest) {
   // int8 input is version 2.
   resize_nearest_neighbor_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&resize_nearest_neighbor_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_nearest_neighbor_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
@@ -1184,44 +1184,44 @@ TEST(OpVersionTest, VersioningResizeNearestNeighborTest) {
   // int16 input is version 4.
   resize_nearest_neighbor_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&resize_nearest_neighbor_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&resize_nearest_neighbor_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
 }
 TEST(OpVersionTest, VersioningAbsTest) {
   // Default.
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_ABS,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*op=*/BuiltinOperator_ABS,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   // int8 input is version 2.
   fake_op_sig = {
-      .op = BuiltinOperator_ABS,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_ABS,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   // int16 quantized input is version 3.
   fake_op_sig = {
-      .op = BuiltinOperator_ABS,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_ABS,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   fake_op_sig.ext_options.abs.input_quantized = true;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
   // int16 non-quantized input is version 4.
   fake_op_sig = {
-      .op = BuiltinOperator_ABS,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_ABS,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
   fake_op_sig = {};
@@ -1249,52 +1249,52 @@ TEST(OpVersionTest, VersioningBatchMatMulTest) {
   // Default.
   TfLiteBatchMatMulParams batch_mat_mul_params = {};
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_BATCH_MATMUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_BATCH_MATMUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&batch_mat_mul_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&batch_mat_mul_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   // int8 input is version 2.
   batch_mat_mul_params = {};
   fake_op_sig = {
-      .op = BuiltinOperator_BATCH_MATMUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_BATCH_MATMUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .builtin_data = reinterpret_cast<void*>(&batch_mat_mul_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*builtin_data=*/reinterpret_cast<void*>(&batch_mat_mul_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   // int16 input is version 3.
   fake_op_sig = {
-      .op = BuiltinOperator_BATCH_MATMUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_BATCH_MATMUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt16, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .builtin_data = reinterpret_cast<void*>(&batch_mat_mul_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*builtin_data=*/reinterpret_cast<void*>(&batch_mat_mul_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   // Symmetric hybrid quantized input is version 1.
   fake_op_sig = {
-      .op = BuiltinOperator_BATCH_MATMUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_BATCH_MATMUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&batch_mat_mul_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&batch_mat_mul_params),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   // Asymmetric hybrid quantized input is version 4.
   fake_op_sig = {
-      .op = BuiltinOperator_BATCH_MATMUL,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_BATCH_MATMUL,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .builtin_data = reinterpret_cast<void*>(&batch_mat_mul_params),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*builtin_data=*/reinterpret_cast<void*>(&batch_mat_mul_params),
   };
   batch_mat_mul_params.asymmetric_quantize_inputs = true;
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 4);
@@ -1302,19 +1302,19 @@ TEST(OpVersionTest, VersioningBatchMatMulTest) {
 TEST(OpVersionTest, VersioningSquaredDifferenceTest) {
   // Default.
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_SQUARED_DIFFERENCE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SQUARED_DIFFERENCE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteFloat32, kTfLiteFloat32}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
 
   // int8 input is version 2.
   fake_op_sig = {
-      .op = BuiltinOperator_SQUARED_DIFFERENCE,
-      .inputs = CreateOpSignatureTensorSpecs(
+      /*op=*/BuiltinOperator_SQUARED_DIFFERENCE,
+      /*inputs=*/CreateOpSignatureTensorSpecs(
           std::vector<TfLiteType>{kTfLiteInt8, kTfLiteInt8}),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 }
@@ -1332,24 +1332,24 @@ TEST(OpVersionTest, VersioningRsqrtTest) {
 }
 TEST(OpVersionTest, VersioningBroadcastToTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_BROADCAST_TO,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*op=*/BuiltinOperator_BROADCAST_TO,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 
   // Quantized broadcast_to op is version 3.
   fake_op_sig = {
-      .op = BuiltinOperator_BROADCAST_TO,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_BROADCAST_TO,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 
   fake_op_sig = {
-      .op = BuiltinOperator_BROADCAST_TO,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
-      .outputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_BROADCAST_TO,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*outputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 3);
 }
@@ -1392,18 +1392,18 @@ TEST(OpVersionTest, VersioningUnidirectionalLstmTest) {
 
 TEST(OpVersionTest, VersioningExpTest) {
   OpSignature fake_op_sig = {
-      .op = BuiltinOperator_EXP,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),
+      /*op=*/BuiltinOperator_EXP,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteFloat32),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 1);
   fake_op_sig = {
-      .op = BuiltinOperator_EXP,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt8),
+      /*op=*/BuiltinOperator_EXP,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt8),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
   fake_op_sig = {
-      .op = BuiltinOperator_EXP,
-      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt16),
+      /*op=*/BuiltinOperator_EXP,
+      /*inputs=*/CreateOpSignatureTensorSpecs(kTfLiteInt16),
   };
   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 2);
 }
diff --git a/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc b/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc
index c4331e099a2..928e0c01148 100644
--- a/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc
+++ b/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc
@@ -274,8 +274,9 @@ void Avx2MatrixBatchVectorMultiplyAccumulateImpl(
       // Horizontally add the 4 intermediate sum values to get the final
       // dot-prod value for this row.
       int32_t sum = ReduceInt32x4(dotprod_32x4);
-
+#ifdef __clang__
 #pragma clang loop unroll(disable) vectorize(disable)
+#endif
       // Postamble loop for <4x remaining 8-bit inputs.
       for (; col < m_cols; ++col) {
         sum += row_ptr[col] * vectors[col];
diff --git a/tensorflow/lite/minimal_logging_default.cc b/tensorflow/lite/minimal_logging_default.cc
index 72632174a54..c36ee9cf21d 100644
--- a/tensorflow/lite/minimal_logging_default.cc
+++ b/tensorflow/lite/minimal_logging_default.cc
@@ -34,10 +34,14 @@ void MinimalLogger::LogFormatted(LogSeverity severity, const char* format,
                                  va_list args) {
   if (severity >= MinimalLogger::minimum_log_severity_) {
     fprintf(stderr, "%s: ", GetSeverityName(severity));
+#ifdef __clang__
 #pragma clang diagnostic push
 #pragma clang diagnostic ignored "-Wformat-nonliteral"
+#endif
     vfprintf(stderr, format, args);
+#ifdef __clang__
 #pragma clang diagnostic pop
+#endif
     fputc('\n', stderr);
   }
 }
